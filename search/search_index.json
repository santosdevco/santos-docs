{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Visi\u00f3n General de la Documentaci\u00f3n","text":"<p>Bienvenido a la documentaci\u00f3n de SantosDev. Aqu\u00ed encontrar\u00e1s una colecci\u00f3n organizada de laboratorios, tutoriales y proyectos pr\u00e1cticos, estructurados para que puedas aprovechar r\u00e1pidamente los ejemplos basados en Python y Kubernetes. Cada secci\u00f3n comienza con un breve p\u00e1rrafo introductorio que explica el prop\u00f3sito y el alcance de los contenidos all\u00ed publicados.</p>"},{"location":"#proyectos-y-laboratorios-en-python","title":"\ud83d\udc0d Proyectos y laboratorios en Python","text":"<p>En esta secci\u00f3n se agrupan todos los ejemplos y proyectos enfocados en el ecosistema Python. Encontrar\u00e1s laboratorios paso a paso para integrar Celery con FastAPI, as\u00ed como soluciones pr\u00e1cticas de scraping y proyectos de Recuperaci\u00f3n de Im\u00e1genes Basada en Contenido (CBIR).</p> <ul> <li>Introducci\u00f3n a Celery + FastAPI Labs   Estos cinco laboratorios muestran c\u00f3mo construir aplicaciones escalables y asincr\u00f3nicas en Python combinando FastAPI y Celery. Aprender\u00e1s desde la configuraci\u00f3n b\u00e1sica hasta el uso de WebSockets para notificaciones en tiempo real.  </li> <li>CeleryFastAPILabs (p\u00e1gina principal) </li> <li>Lab 1 \u2013 FastAPI + Celery + Redis (b\u00e1sico) </li> <li>Lab 2 \u2013 Arquitectura escalable + Alembic </li> <li>Lab 3 \u2013 Roles avanzados y m\u00f3dulo de chatroom </li> <li>Lab 4 \u2013 Chat en tiempo real con WebSocket </li> <li> <p>Lab 5 \u2013 Notificaciones WebSocket de tareas Celery</p> </li> <li> <p>Google Maps Scraper   Este proyecto en Python automatiza la recolecci\u00f3n de datos de Google Maps, almacena la informaci\u00f3n en MongoDB y genera reportes en Excel. Ideal para aprender t\u00e9cnicas de scraping con <code>requests</code> y <code>BeautifulSoup</code>, as\u00ed como el manejo de bases de datos NoSQL.  </p> </li> <li> <p>GoogleMapsScrapper</p> </li> <li> <p>CBIR Project (Detecci\u00f3n de enfermedades en plantas)   Aplicaci\u00f3n distribuida basada en microservicios Docker para realizar Content-Based Image Retrieval. Incluye extracci\u00f3n de descriptores, construcci\u00f3n de \u00edndices y b\u00fasqueda de im\u00e1genes similares. Una soluci\u00f3n pr\u00e1ctica para reconocimiento de enfermedades en plantas mediante visi\u00f3n por computador.  </p> </li> <li>CBIR_Project</li> </ul>"},{"location":"#laboratorios-de-kubernetes","title":"\u2638\ufe0f Laboratorios de Kubernetes","text":"<p>En esta secci\u00f3n encontrar\u00e1s ejemplos detallados para implementar y gestionar cl\u00fasteres de Kubernetes, tanto en entornos locales con Minikube como en producci\u00f3n con Azure Kubernetes Service (AKS). Cada laboratorio est\u00e1 dise\u00f1ado para ilustrar conceptos clave paso a paso.</p> <ul> <li>Introducci\u00f3n a los laboratorios de Minikube   Aprende a configurar un entorno local de Kubernetes usando Minikube y a desplegar aplicaciones Dockerizadas. Este bloque de laboratorios cubre desde la instalaci\u00f3n de Minikube hasta la puesta en producci\u00f3n de aplicaciones y servicios internos.  </li> <li>Minikube (p\u00e1gina principal) </li> <li>Lab 1 \u2013 LoadBalancer + API Flask </li> <li>Lab 2 \u2013 Vol\u00famenes compartidos (<code>emptyDir</code>) </li> <li>Lab 3 \u2013 Comunicaci\u00f3n interna entre containers </li> <li>Lab 4 \u2013 Service NodePort entre Pods </li> <li> <p>Lab 5 \u2013 Docker Coins en Kubernetes</p> </li> <li> <p>Introducci\u00f3n a AKS (Azure Kubernetes Service)   En este laboratorio aprender\u00e1s a crear un Container Registry en Azure, generar un cl\u00faster de AKS y desplegar la aplicaci\u00f3n Docker Coins con las mismas configuraciones usadas en Minikube. Una gu\u00eda paso a paso para llevar tus servicios a la nube de Microsoft.  </p> </li> <li>AKS Lab 1</li> </ul>"},{"location":"GoogleMapsScrapper/","title":"Google Maps Scraper","text":"<p>El prop\u00f3sito de este proyecto es automatizar la extracci\u00f3n de informaci\u00f3n espec\u00edfica de Google Maps. Durante la primera fase, se desarrolla un proyecto en Python que toma instrucciones sobre las zonas y las consultas que debe realizar. Posteriormente, extrae la informaci\u00f3n requerida y la almacena en una base de datos MongoDB. Finalmente, se genera un archivo Excel con la informacion solicitada.</p>"},{"location":"GoogleMapsScrapper/#pasos-para-usar-el-proyecto","title":"Pasos para usar el proyecto:","text":""},{"location":"GoogleMapsScrapper/#1-descargar-el-proyecto","title":"1. Descargar el proyecto","text":"<pre><code>git clone https://github.com/SantiagoAndre/google-maps-scrapping\n</code></pre>"},{"location":"GoogleMapsScrapper/#2-crear-la-imagen-de-docker","title":"2. Crear la imagen de Docker:","text":"<pre><code>cd google-maps-scrapping\ndocker build . -t santosdev20/googlemapsscrapping:v9\n</code></pre>"},{"location":"GoogleMapsScrapper/#2-crear-archivos-de-configuracion","title":"2. Crear archivos de configuraci\u00f3n:","text":"<p>Antes de ejecutar el proyecto, es necesario crear dos archivos: <code>queries.json</code> y <code>.env</code>.</p> <p><code>queries.json</code>:</p> <p>Este archivo define las consultas de localizaciones para el scraper. Debe tener el siguiente formato:</p> <pre><code>[\n    {\n        \"name\": \"PENNSYLVANIA\",\n        \"countries\": [\n            {\n                \"name\": \"JEFFERSON COUNTY\",\n                \"zips\": [\n                    19140\n                ]\n            }\n        ]\n    }\n]\n</code></pre> <p>El archivo consiste en una lista de estados, cada estado tiene sus pa\u00edses y cada pa\u00eds tiene sus c\u00f3digos postales (zipcodes).</p> <p><code>.env</code>:</p> <p>Este archivo contiene las variables de entorno que necesita el scraper. A continuaci\u00f3n se presentan las variables requeridas:</p> <p><pre><code>CONTACTACTS_DB=contacts_database\nCONTACTS_COLLECTION=contacts\nLINKS_COLLECTION=links\nINDUSTRIES_COLLECTION=industries\nSYNC_M_COLLECTION=sync_collection\nMONGO_CONNECTION=mongodb://root:example@mongo_mongo_1:27017\nENVIRONMENT=prod\n</code></pre> la variable mongo conection debe ir con la cadena de conexion a la base de datos productiva.</p>"},{"location":"GoogleMapsScrapper/#3-correr-el-proyecto","title":"3. Correr el proyecto:","text":"<p>Para ejecutar el proyecto, primero da permisos de ejecuci\u00f3n al script:</p> <pre><code>chmod +x run.sh   # Para Linux\n</code></pre> <p>Luego, ejecuta el script correspondiente:</p> <p><pre><code>./run.sh  [queriespath] [env path] [outputspath] [version]         # Para Linux\n</code></pre> o <pre><code>./run.bat [queriespath] [env path] [outputspath] [version]         # Para Windows\n</code></pre></p> <p>Por ejemplo </p> <pre><code>./run.sh ./queriesExample.json .envExample ./outputs v1\n# Estoy pasando el archivo queriesExample.json con la informacion de las zonas que quiero cubrir\n# los archivos de excel quedaran en la carpeta outputs\n# Estoy pasandole el archivo .envExample con las variables de entorno\n# Estoy usando la v1 del proyecto.\n</code></pre> <p>El proyecto empezara a correr y mostrara logs de todo lo que esta haciendo, al final en la carpeta outputs estaran todos los excels generados</p>"},{"location":"cbir_project/","title":"Microservices with docker","text":"<ul> <li>Project name: PLANT DISEASE DETECTION APPLICATION </li> <li>Year: 2020</li> <li>Technologies: Fast API, Flask, Vuejs, Docker, docker-compose, microservices</li> <li>Repo: Repo</li> </ul> <p>This is a  distributed application for detect plants diseases, analyzing its leafs with cbir algorithms.</p> <p>With this project you can learn a little about microservices and docker, this project is not fixed to production but is a base to the next step,  use Kubernetes to inprove the comunication between each mecroservices through Services like the lab docker coins.</p> <p>Watch video demo in this link: Demo.</p>"},{"location":"cbir_project/#content-based-image-retrieval-cbir","title":"Content-Based Image Retrieval (CBIR)","text":"<p>The 4 Steps of Any CBIR System</p> <p>No matter what Content-Based Image Retrieval System you are building, they all can be boiled down into 4 distinct steps:</p> <ol> <li>Defining your image descriptor: At this phase, you need to decide what aspect of the image you want to describe. Are you interested in the color of the image? The shape of an object in the image? Or do you want to characterize texture?</li> <li>Indexing your dataset: Now that you have your image descriptor defined, your job is to apply this image descriptor to each image in your dataset, extract features from these images, and write the features to storage (ex. CSV file, RDBMS, Redis, etc.) so that they can be later compared for similarity.</li> <li>Defining your similarity metric: Cool, now you have a bunch of feature vectors. But how are you going to compare them? Popular choices include the Euclidean distance, Cosine distance, and chi-squared distance, but the actual choice is highly dependent on (1) your dataset and (2) the types of features you extracted.</li> <li>Searching: The final step is to perform an actual search. A user will submit a query image to your system (from an upload form or via a mobile app, for instance) and your job will be to (1) extract features from this query image and then (2) apply your similarity function to compare the query features to the features already indexed. From there, you simply return the most relevant results according to your similarity function.</li> </ol>"},{"location":"cbir_project/#architecture-and-flow","title":"Architecture and flow","text":""},{"location":"cbir_project/#container-diagram","title":"Container diagram","text":"<p>This application is composed of Five microservices, each one complies with a specific responsibility of Content-Based Image Retrieval (CBIR) Flow. </p> <ol> <li>Descriptor Microservice: This microservice had one responsibility, describe an input image,  in other words, extract vectors, matrix of image with algorithms of DESCRIPTOR_DIC list. For this example, we use:</li> <li>Color descriptor</li> <li>Texture descriptor</li> <li>Color Tree</li> <li>SIFT</li> <li> <p>Storage Microservice: This microservice had two responsibilities, save image and vector descriptors, and query images from a features vector of input.</p> </li> <li> <p>Match Microservice: This microservice had one responsibility, compare two features vectors of two images, and returns match percentage.</p> </li> <li> <p>Cbir Microservice: This microservice fulfits the role of api and main functions two(view flow diagram).</p> </li> <li> <p>Frontend Microservice: This small Microservice have two interfaces</p> <ul> <li>Upload photos and her diseases</li> <li>Browser similar photos of a leaf plant image, with its possible disease.</li> </ul> </li> </ol>"},{"location":"cbir_project/#sequence-diagram","title":"Sequence diagram","text":""},{"location":"cbir_project/#how-to-run-this-project","title":"How to run this project","text":"<p>this project is dockerized, then is easy to use it with docker-compose, to run this project use this command</p> <pre><code>cd src\ndocker-compose up \n</code></pre> <pre><code>version: \"3.7\"\nservices:\n  db:\n    image: mongo\n    volumes:\n      - $PWD/db_data:/data/db\n  storage:\n    build: storage\n    environment:\n      MONGO_URI: \"mongodb://db:27017/cbir\"\n      MATCHER_URL: \"http://matcher:8002/\"\n      UPLOAD_FOLDER: \"static/uploads\"\n      PYTHONUNBUFFERED: 1\n    volumes:\n      - ./storage/static/:/app/static\n      - ./storage/src:/app/src\n    depends_on:\n      - db\n  api:\n    build: api\n    environment:\n      STORAGE_URL: \"http://storage:5000/\"\n      DESCRIPTOR_URL: \"http://descriptor:8001/\"\n      PYTHONUNBUFFERED: 1\n    volumes:\n      - type: bind\n        source: ./api/src\n        target: /app/src\n    links:\n      - storage\n    depends_on:\n      - storage\n\n  descriptor:\n    build: descriptor\n    environment:\n      PYTHONUNBUFFERED: 1\n    volumes:\n      - ./descriptor/src:/app/src\n    depends_on:\n      - db\n  matcher:\n    build: matcher\n    environment:\n      PYTHONUNBUFFERED: 1\n    volumes:\n      - ./matcher/src:/app/src\n    depends_on:\n      - db\n  frontend:\n    build: frontend\n    ports:\n      - \"8080:8080\"\n    environment:\n      VUE_APP_API_URL: \"http://api:8000/\" \n    volumes:\n      - ./frontend/:/app\n        ./frontend/node_modules\n\n    links:\n      - api\n</code></pre>"},{"location":"haskell/","title":"Lists In Haskell","text":"<ul> <li>Year: 2019</li> <li>Technologies: Programacion funcional con Haskell.</li> <li>Repo: Repo</li> </ul> <p>Este proyecto lista de pacientes, permitiendo realizar varias operaciones como agregar, buscar, modificar y eliminar pacientes. Adem\u00e1s, calcula el IMC de cada paciente y los clasifica en categor\u00edas como peso normal, sobrepeso u obesidad.</p>"},{"location":"haskell/#modulos-y-estructuras-de-datos","title":"M\u00f3dulos y Estructuras de Datos","text":"<p>El proyecto consta de varios m\u00f3dulos y estructuras de datos principales:</p> <ol> <li>Tipos de Datos:</li> <li><code>Genero</code>: Representa el g\u00e9nero del paciente.</li> <li><code>DiagnosticoIMC</code>: Enumeraci\u00f3n para las categor\u00edas de IMC.</li> <li> <p><code>Paciente</code>: Estructura para almacenar la informaci\u00f3n del paciente.</p> </li> <li> <p>Funciones Principales:</p> </li> <li><code>imcCalcule</code>: Calcula el IMC.</li> <li><code>diagnosticoIMC</code>: Determina la categor\u00eda del IMC.</li> <li> <p><code>main</code>: Funci\u00f3n principal que inicia la aplicaci\u00f3n.</p> </li> <li> <p>Acciones del Men\u00fa:</p> </li> <li>Funciones para agregar, buscar, alertar sobre pacientes con IMC espec\u00edficos, modificar, eliminar e imprimir informaci\u00f3n de pacientes.</li> </ol>"},{"location":"haskell/#diagrama-de-flujo","title":"Diagrama de flujo","text":""},{"location":"haskell/#implementacion-detallada","title":"Implementaci\u00f3n Detallada","text":"<ol> <li>Definici\u00f3n de Tipos de Datos:</li> <li>Los tipos <code>Genero</code> y <code>DiagnosticoIMC</code> usan el sistema de tipos algebraicos de Haskell para definir enumeraciones.</li> <li> <p>La estructura <code>Paciente</code> usa el tipo de datos <code>data</code> para definir un registro.</p> </li> <li> <p>C\u00e1lculo del IMC y Diagn\u00f3stico:</p> </li> <li><code>imcCalcule</code> toma peso y altura para calcular el IMC.</li> <li> <p><code>diagnosticoIMC</code> clasifica el IMC en categor\u00edas.</p> </li> <li> <p>Funci\u00f3n Principal (<code>main</code>):</p> </li> <li> <p>Inicia la aplicaci\u00f3n y maneja el flujo del men\u00fa principal.</p> </li> <li> <p>Acciones del Men\u00fa:</p> </li> <li><code>agregarPaciente</code>: Agrega un nuevo paciente a la lista.</li> <li><code>buscarPaciente</code>: Busca un paciente por c\u00e9dula.</li> <li><code>buscarAlertas</code>: Encuentra pacientes con IMC en categor\u00edas espec\u00edficas.</li> <li><code>modificarPacienteIO</code>: Modifica el peso o la estatura de un paciente.</li> <li><code>eliminarPaciente</code>: Elimina un paciente de la lista.</li> <li> <p><code>imprimirPacientes</code>: Muestra todos los pacientes.</p> </li> <li> <p>Funciones Auxiliares:</p> </li> <li>Funciones para seleccionar g\u00e9nero, peso, estatura y manejo de opciones del men\u00fa.</li> </ol>"},{"location":"haskell/#uso-y-ejecucion","title":"Uso y Ejecuci\u00f3n","text":"<p>Para usar el programa, el usuario interact\u00faa con un men\u00fa de consola que permite realizar las diferentes operaciones. La aplicaci\u00f3n mantiene una lista de pacientes en memoria, que se modifica con cada operaci\u00f3n.</p>"},{"location":"haskell/#observaciones-adicionales","title":"Observaciones Adicionales","text":"<ul> <li>El c\u00f3digo hace un uso intensivo de funciones puras y manejo de IO en Haskell.</li> <li>Se utiliza pattern matching y funciones de orden superior para operaciones sobre listas.</li> </ul>"},{"location":"haskell/#conclusion","title":"Conclusi\u00f3n","text":"<p>Este proyecto demuestra una aplicaci\u00f3n pr\u00e1ctica del lenguaje funcional Haskell en la gesti\u00f3n de datos de pacientes y c\u00e1lculo de IMC. Combina conceptos de programaci\u00f3n funcional como inmutabilidad, tipos de datos algebraicos y manejo de entrada/salida.</p>"},{"location":"versions/","title":"Versions: Clone of Git made in C","text":"<ul> <li>A\u00f1o: 2018</li> <li>Repo: https://gitlab.com/santiagoandre/versions</li> </ul> <p>Es un proyecto basico de un sistema de control de versiones, que incluye:   - Adicionar un archivo al repositorio de versiones.   - Listar las versiones de un archivo.   - Recuperar una version anterior de un archivo. En esta implementaci\u00f3n no es recursiva por lo cual s\u00f3lo se pueden agregar archivos que se encuentren en el directorio desde el cual se invoque el comando.</p>"},{"location":"versions/#como-funciona","title":"Como funciona:","text":"<p>Al invocar el comando versions por primera vez, se  crea un directorio llamado .versions. Se crea un sub-directorio por cada archivo agregado al repositorio. Dentro de este sub-directorio se almacenan las diferentes versiones del archivo, y un archivo llamado versions.txt que contiene la descripci\u00f3n de cada versi\u00f3n.</p>"},{"location":"versions/#diagrama-de-flujo","title":"Diagrama de flujo","text":""},{"location":"versions/#requerimientos","title":"Requerimientos:","text":"<p>En las distribuciones basadas en debian se necesita el siguiente paquete  libssl-dev <pre><code> sudo apt install libssl-dev\n</code></pre></p>"},{"location":"versions/#compilar","title":"Compilar","text":"<p>Anadir la bandera -lcrypto  <pre><code> gcc -o versions versions.c hash/openssl_hash.c -lcrypto\n</code></pre></p>"},{"location":"versions/#uso","title":"Uso:","text":"<p>Agregar una version de un archivo:</p> <p><pre><code>versions add archivo \"Comentario sobre la version\"\n</code></pre> Listar las verciones de un archivo: <pre><code>versions list archivo\n</code></pre> recuperar una version de un archivo</p> <pre><code>versions get num_ver archivo\n</code></pre>"},{"location":"Azurecertifications/Az204/","title":"Azure 204 Certification","text":""},{"location":"Azurecertifications/Az204/#amazing-microsoft-resources","title":"Amazing Microsoft Resources","text":"<p>Here are some official Microsoft websites that you really should bookmark when studying for the AZ-204 exam:</p> <ul> <li> <p>Learning paths on MS Learn</p> </li> <li> <p>Azure Code Samples</p> </li> <li> <p>Official Azure Documentation</p> </li> <li>Official Microsoft Azure YouTube Channel</li> <li>Official Microsoft Developer YouTube Channel</li> <li>Azure REST API Browser</li> </ul>"},{"location":"Azurecertifications/Az204/#microsoft-labs-and-workshops-practice-is-the-key-to-success","title":"Microsoft Labs and Workshops - Practice is the key to success","text":"<ul> <li> <p>Azure Citadel - Labs and Workshops</p> </li> <li> <p>Microsoft Cloud Workshop - More labs and workshops</p> </li> <li> <p>Github AZ-204 from Microsoft Training</p> </li> </ul>"},{"location":"Azurecertifications/Az204/1.vms/","title":"Virtual Machines","text":"<p>Azure Virtual Machines (VM) is one of several types of <code>on-demand</code>, scalable computing resources that Azure offers. Typically, you choose a VM when you need more control over the computing environment than the other choices offer. This article gives you information about what you should consider before you create a VM, how you create it, and how you manage it.</p> <p>When you create one vm in azure you can choose many things: - Operative System(Image Base) - Size(There are several options on azure for diferents porpuses)(Ram, CPU, Disk) - Availability Strategy</p>"},{"location":"Azurecertifications/Az204/1.vms/#azure-domains","title":"Azure Domains","text":"<p>There are two azure domains types:</p>"},{"location":"Azurecertifications/Az204/1.vms/#fault-domains","title":"Fault Domains","text":"<p>Fault domains define the group of virtual machines that share a common power source and network switch. By default, the virtual machines configured within your availability set are separated across up to three fault domains</p>"},{"location":"Azurecertifications/Az204/1.vms/#update-domains","title":"Update Domains","text":"<p>Update domains indicate groups of virtual machines and underlying physical hardware that can be rebooted at the same time. When more than five virtual machines are configured within a single availability set with five update domains, the sixth virtual machine is placed into the same update domain as the first virtual machine, the seventh in the same update domain as the second virtual machine, and so on. The order of update domains being rebooted may not proceed sequentially during planned maintenance, but only one update domain is rebooted at a time. A rebooted update domain is given 30 minutes to recover before maintenance is initiated on a different update domain.</p>"},{"location":"celeryfastapi/CeleryFastAPILabs/","title":"FastAPI &amp; Celery  Labs","text":"<p>Bienvenido a mi colecci\u00f3n de laboratorios pr\u00e1cticos dise\u00f1ados para llevarlo de cero a intermedio en el uso de FastAPI con Celery. Este repositorio contiene cinco laboratorios progresivos, cada uno de los cuales se basa en los conocimientos y habilidades desarrollados en el anterior.</p>"},{"location":"celeryfastapi/CeleryFastAPILabs/#nota-importante","title":"Nota importante","text":"<p>Este proyecto tiene su proyecto completo en Github, y cada laboratorio tiene su documentacion, pero aun no he liberado la documentacion paso a paso estara colocada en subsecciones de este apartado.</p>"},{"location":"celeryfastapi/CeleryFastAPILabs/#descripcion-general","title":"Descripci\u00f3n general","text":"<p>Estos laboratorios ofrecen un enfoque pr\u00e1ctico para aprender la integraci\u00f3n de FastAPI, un framework r\u00e1pido para crear API\u2019s con Python, con Celery, un potente sistema de cola de tareas distribuidas. Cada laboratorio est\u00e1 estructurado para mejorar su comprensi\u00f3n de manera incremental, desde conceptos b\u00e1sicos hasta implementaciones m\u00e1s avanzadas. Cada laboratorio tiene su propio README y diagramas para entender f\u00e1cilmente cada uno, todos los laboratorios estan montados sobre Docker y Docker-Compose para ejecutar cada uno en su m\u00e1quina sin ning\u00fan problema.</p>"},{"location":"celeryfastapi/CeleryFastAPILabs/#resumenes-de-laboratorio","title":"Res\u00famenes de laboratorio","text":"<ul> <li> <p>Laboratorio 1: Aplicaci\u00f3n b\u00e1sica de apio FastAPI    Introducci\u00f3n a FastAPI, configuraci\u00f3n de una API sencilla.</p> </li> <li> <p>Laboratorio 2: Integraci\u00f3n de Project Factory y SqlAlChemy    Refactorizaci\u00f3n a una estructura m\u00e1s escalable e integraci\u00f3n SQLAlCHemy</p> </li> <li> <p>Laboratorio 3: M\u00f3dulo de Gesti\u00f3n de Usuarios y Salas de Chat    Implementaci\u00f3n de roles de usuario, decoradores de rutas para protegerlos y una funci\u00f3n de sala de chat.</p> </li> <li> <p>Laboratorio 4: Aplicaci\u00f3n de chat en tiempo real    Mejora de la aplicaci\u00f3n de la sala de chat con capacidades de mensajer\u00eda en tiempo real.</p> </li> <li> <p>Laboratorio 5: WebSocket con Apio    Incorporaci\u00f3n de WebSocket para actualizaciones en vivo sobre los estados de las tareas de Celery.</p> </li> </ul>"},{"location":"celeryfastapi/CeleryFastAPILabs/#instalacion","title":"Instalaci\u00f3n","text":"<p>Debe tener Docker y Docker-Compose configurados en su m\u00e1quina. Para ejecutar cada laboratorio, debe seguir los siguientes pasos:</p> <ol> <li><code>cd labfolfer</code></li> <li><code>docker-componer</code></li> </ol> <p>Nota: si el laboratorio tiene modelos de base de datos, despu\u00e9s del \u00faltimo comando hay que ejecutar</p> <pre><code>docker-compose exec web bash\nalembic revision --autogenerate\nalembic upgrade head\n</code></pre> <p>Estos comandos migran los modelos a la base de datos.</p>"},{"location":"celeryfastapi/lab1/","title":"Lab 1: FastAPI Integration with Celery and Redis","text":"<p>The objective of this lab is to demonstrate a basic integration of FastAPI with Celery and Redis for asynchronous task execution.</p>"},{"location":"celeryfastapi/lab1/#designs","title":"Designs","text":""},{"location":"celeryfastapi/lab1/#container-design","title":"Container Design","text":""},{"location":"celeryfastapi/lab1/#flow-diagram","title":"FLow Diagram","text":""},{"location":"celeryfastapi/lab1/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Docker Compose </li> </ul>"},{"location":"celeryfastapi/lab1/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 Dockerfile.dev\n\u251c\u2500\u2500 .env.dev\n\u251c\u2500\u2500 README.MD\n\u251c\u2500\u2500 scripts/start_flower.sh\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"celeryfastapi/lab1/#description","title":"Description","text":"<ul> <li><code>main.py</code>: Contains the definition of a basic FastAPI API and a Celery task named <code>divide</code>.</li> <li><code>Dockerfile.dev</code>: Specifies how to build the Docker image for the FastAPI application and the Celery worker.</li> <li><code>.env.dev</code>: Configuration file that defines the environment variables used in <code>main.py</code>.</li> <li><code>docker-compose.yaml</code>: Defines the necessary services to run the application and the Celery task.</li> <li><code>scripts/start_flower.sh</code>: Has a code to start the flower container.</li> </ul>"},{"location":"celeryfastapi/lab1/#steps-to-run-the-lab","title":"Steps to Run the Lab","text":"<ol> <li>Build and run services with Docker Compose</li> </ol> <pre><code>docker-compose up --build\n</code></pre> <ol> <li>Access the FastAPI API</li> </ol> <p>Once the services are up and running, you can access the FastAPI API at <code>http://localhost:8010</code>.</p> <ol> <li>Trigger a Celery Task</li> </ol> <p>You can invoke the <code>divide</code> task by making a GET request to the <code>/divide-celery</code> route with parameters <code>a</code> and <code>b</code>. For example:</p> <pre><code>http://localhost:8010/divide-celery?a=10&amp;b=2\n</code></pre> <p>The API will respond with a <code>task_id</code> which you can use to query the status and result of the task.</p> <ol> <li>Query the status of a task</li> </ol> <p>Use the <code>/query-task-status</code> route with the <code>task_id</code> obtained from the previous step to check the status and result of the task:</p> <pre><code>http://localhost:8010/query-task-status?task_id=YOUR_TASK_ID_HERE\n</code></pre>"},{"location":"celeryfastapi/lab1/#flower","title":"Flower","text":"<p>Flower is a real-time web application monitoring and administration tool for Celery. Navigate to http://localhost:5550 in your browser of choice to view the dashboard. Click \u201cTasks\u201d in the nav bar at the top to view the finished tasks.</p>"},{"location":"celeryfastapi/lab1/#conclusion","title":"Conclusion","text":"<p>This lab showcases how to integrate FastAPI, Celery, and Redis for executing tasks asynchronously. It provides a foundation upon which more advanced and complex labs can be built.</p>"},{"location":"celeryfastapi/lab2/","title":"Lab 2: FastAPI and Celery Integration Documentation","text":"<p>This documentation covers the integration of FastAPI with Celery, alongside tools like SQLAlchemy for database operations and Alembic for migration handling.</p>"},{"location":"celeryfastapi/lab2/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Project Structure</li> <li>Docker Compose Configuration</li> <li>Web Service</li> <li>Redis Service</li> <li>Celery Worker Service</li> <li>Flower Monitoring Service</li> <li>Database Service</li> <li>Setting Up and Running</li> </ol> <p>Certainly! Here\u2019s a dedicated section that explains the new structure of your project.</p>"},{"location":"celeryfastapi/lab2/#designs","title":"Designs:","text":""},{"location":"celeryfastapi/lab2/#user-perspective","title":"User perspective","text":""},{"location":"celeryfastapi/lab2/#developer-perspective","title":"Developer perspective","text":""},{"location":"celeryfastapi/lab2/#flow-diagram","title":"Flow diagram","text":"<p>This app provides a simple api to manage users, when a new user is created, a new task to send registration email is added to celery queue</p> <p></p>"},{"location":"celeryfastapi/lab2/#1-project-structure-overview","title":"1. Project Structure Overview","text":"<p>In this laboratory exercise, we\u2019ve introduced a more framework-centric structure that promotes modular design, scalability, and ease of maintenance. Let\u2019s break down the different components:</p>"},{"location":"celeryfastapi/lab2/#directory-structure","title":"Directory Structure:","text":"<pre><code>.\n\u251c\u2500\u2500 alembic/\n\u251c\u2500\u2500 project/\n\u2502   \u251c\u2500\u2500 database.py\n\u2502   \u251c\u2500\u2500 celery_utils.py\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u2514\u2500\u2500 config.py\n\u251c\u2500\u2500 scripts/\n\u251c\u2500\u2500 .env.dev\n\u251c\u2500\u2500 Dockerfile.dev\n\u251c\u2500\u2500 docker-compose.yaml\n|   alembic.ini\n\u251c\u2500\u2500 main.py\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"celeryfastapi/lab2/#descriptions","title":"Descriptions:","text":"<ol> <li>alembic/: </li> <li> <p>This directory is dedicated to Alembic, our database migration tool. It contains migration scripts, configurations, and version history. This helps in maintaining a versioned history of our database schema changes.</p> </li> <li> <p>project/:</p> </li> <li>database.py: This module sets up and configures our database connection and session using SQLAlchemy.</li> <li>celery_utils.py: Utility for setting up and configuring the Celery instance for background task processing.</li> <li>users/: A module-based directory focusing on user-related operations. This promotes modular design where each module (like users) can have its own models, views, and controllers.<ul> <li>schemas.py: Pydantic schemas for user-related requests and responses.</li> <li>tasks.py: Celery tasks related to the users module.</li> <li>crud.py: CRUD operations for the user model.</li> <li>models.py: SQLAlchemy models for users.</li> </ul> </li> <li> <p>config.py: Configuration settings for the project, making use of environment variables or default values.</p> </li> <li> <p>scripts/:</p> </li> <li> <p>Contains shell scripts which might be useful for bootstrapping, deployment, or task automation. For instance, <code>start_flower.sh</code> is present to bootstrap the Flower monitor for Celery.</p> </li> <li> <p>.env.dev:</p> </li> <li> <p>Environment-specific (development in this case) variables are stored here. They are used by Docker and can also be read within the application to configure certain settings.</p> </li> <li> <p>Dockerfile.dev:</p> </li> <li> <p>The Docker configuration file for creating a development environment container for our application.</p> </li> <li> <p>docker-compose.yaml:</p> </li> <li> <p>Docker Compose configuration that helps in defining and running multi-container Docker applications. This is where services like the web application, database, Celery worker, and others are defined.</p> </li> <li> <p>main.py:</p> </li> <li>The entry point to our FastAPI application. This is where the application instance is created and routes are included.</li> </ol>"},{"location":"celeryfastapi/lab2/#2-docker-compose-configuration","title":"2. Docker Compose Configuration","text":"<p>The <code>docker-compose.yaml</code> file contains definitions for various services:</p> <ul> <li>web: The main FastAPI application.</li> <li>redis: The Redis broker for Celery.</li> <li>celery_worker: The Celery worker service.</li> <li>flower: A monitoring tool for Celery.</li> <li>db: PostgreSQL database service.</li> </ul>"},{"location":"celeryfastapi/lab2/#3-web-service","title":"3. Web Service","text":"<p>The web service runs the FastAPI application:</p> <ul> <li>Uses the <code>Dockerfile.dev</code> for building the Docker image.</li> <li>Sets the command to run FastAPI with <code>uvicorn</code>.</li> <li>Maps port <code>8010</code> on the host to port <code>8000</code> on the container.</li> <li>Depends on the <code>redis</code> and <code>db</code> services.</li> </ul>"},{"location":"celeryfastapi/lab2/#4-redis-service","title":"4. Redis Service","text":"<p>Redis is used as the message broker for Celery:</p> <ul> <li>Uses the official Redis image (<code>redis:7-alpine</code>).</li> </ul>"},{"location":"celeryfastapi/lab2/#5-celery-worker-service","title":"5. Celery Worker Service","text":"<p>This service processes tasks asynchronously:</p> <ul> <li>Uses the same Docker image as the web service.</li> <li>Runs the Celery worker with the appropriate configurations.</li> <li>Depends on the <code>redis</code> service for task management.</li> </ul>"},{"location":"celeryfastapi/lab2/#6-flower-monitoring-service","title":"6. Flower Monitoring Service","text":"<p>Flower is used to monitor and manage Celery tasks:</p> <ul> <li>Uses the same Docker image as the web service.</li> <li>Runs a shell script to initiate Flower.</li> <li>Maps port <code>5550</code> on the host to <code>5555</code> on the container.</li> <li>Depends on the <code>redis</code> service.</li> </ul>"},{"location":"celeryfastapi/lab2/#7-database-service","title":"7. Database Service","text":"<p>The database service uses PostgreSQL:</p> <ul> <li>Utilizes the <code>postgres:12</code> Docker image.</li> <li>Stores data in a Docker volume named <code>postgres_data</code> to persist data.</li> <li>Sets up the database with provided environment variables (username, password, dbname).</li> </ul>"},{"location":"celeryfastapi/lab2/#8-setting-up-and-running","title":"8. Setting Up and Running","text":"<p>To get the project up and running:</p> <ol> <li>Install Docker and Docker Compose.</li> <li>Navigate to the project directory.</li> <li>Run <code>docker-compose up -d</code> to start all services in detached mode.</li> <li>Access the FastAPI application at <code>http://localhost:8010</code>.</li> <li>Monitor Celery tasks using Flower at <code>http://localhost:5550</code>.</li> </ol>"},{"location":"celeryfastapi/lab2/#9-alembic-database-migrations","title":"9. Alembic: Database Migrations","text":""},{"location":"celeryfastapi/lab2/#introduction","title":"Introduction","text":"<p>Alembic is a lightweight database migration tool for usage with the SQLAlchemy database toolkit. It provides a way to programmatically manage changes in your database schema, ensuring that your database schema is in sync with your SQLAlchemy models. By using migrations, you can maintain a history of changes, which can be very helpful for things like data integrity, database versioning, and collaborative development.</p>"},{"location":"celeryfastapi/lab2/#setup","title":"Setup","text":"<p>Your project already includes an <code>alembic/</code> directory, which contains configuration, migration scripts, and version history. The primary configuration is in <code>alembic.ini</code>, which references <code>alembic/env.py</code> to determine the database connection, dependencies, and more.</p>"},{"location":"celeryfastapi/lab2/#generating-auto-migrations","title":"Generating Auto-Migrations","text":"<ol> <li> <p>Auto-generate Migration Script: After modifying your SQLAlchemy models, you can auto-generate migration scripts by running: <pre><code>alembic revision --autogenerate -m \"Description of the changes\"\n</code></pre> For example, if you added a new column to a user model, you might run: <pre><code>alembic revision --autogenerate -m \"Added age column to user model\"\n</code></pre> This command will detect changes in your models and generate a new migration script under <code>alembic/versions/</code>.</p> </li> <li> <p>Review the Migration Script: It\u2019s a good practice to review the generated migration script to ensure it captures all the changes and to make any necessary tweaks.</p> </li> </ol>"},{"location":"celeryfastapi/lab2/#applying-migrations","title":"Applying Migrations","text":"<ol> <li> <p>Upgrading to the Latest Migration: To apply the migrations and update the database schema, run: <pre><code>alembic upgrade head\n</code></pre> This command will apply all pending migrations up to the latest version.</p> </li> <li> <p>Downgrading Migrations: If you need to undo a migration, you can downgrade. For example, to undo the last migration, you can run: <pre><code>alembic downgrade -1\n</code></pre> To downgrade to a specific migration version, replace <code>-1</code> with the version number (found in the filename of the migration script).</p> </li> </ol>"},{"location":"celeryfastapi/lab3/","title":"Lab 3: Advanced User Management and Chatroom Module","text":"<p>Lab 3 builds upon the work of Lab 2 by introducing advanced user roles and a brand-new chatroom module, enhancing the project\u2019s features and security.</p>"},{"location":"celeryfastapi/lab3/#table-of-contents","title":"Table of Contents","text":"<ol> <li>User Roles</li> <li>Chatroom Module</li> <li>Project Structure</li> <li>Diagram Flow</li> <li>Usage</li> <li>Future Improvements</li> </ol>"},{"location":"celeryfastapi/lab3/#user-roles","title":"User Roles","text":"<p>In Lab 3, we have two user roles: 1. Superuser: Has elevated privileges and can perform actions like adding new users. 2. Normal User: Standard users who can use regular features.</p> <p>New Decorators for Route Protection: - <code>login_required</code>: Ensures the user is logged in. - <code>superuser_required</code>: Ensures the user is a superuser.</p>"},{"location":"celeryfastapi/lab3/#chatroom-module","title":"Chatroom Module","text":"<p>We introduce a chatroom module where users can: - Create new chatrooms. - Add users to chatrooms they own. - View chatrooms they are a part of.</p>"},{"location":"celeryfastapi/lab3/#chatroom-features","title":"Chatroom Features:","text":"<ul> <li>CRUD Operations: Users can create, read, update, and delete chatrooms.</li> <li>Member Management: Superusers can add or remove members from chatrooms.</li> </ul>"},{"location":"celeryfastapi/lab3/#project-structure","title":"Project Structure","text":"<p>The project structure has been updated to reflect the new modules and changes: <pre><code>.\n./main.py\n./alembic\n./project/chatrooms\n./project/users\n... (other files and directories)\n</code></pre></p>"},{"location":"celeryfastapi/lab3/#diagram-flow","title":"Diagram Flow","text":""},{"location":"celeryfastapi/lab3/#user-module","title":"User Module","text":""},{"location":"celeryfastapi/lab3/#chatroom-module_1","title":"Chatroom Module","text":""},{"location":"celeryfastapi/lab3/#usage","title":"Usage","text":"<ol> <li>Creating a Chatroom: <code>POST</code> request to <code>/create/</code> with chatroom details.</li> <li>Viewing Chatrooms: <code>GET</code> request to <code>/</code> to view chatrooms for the logged-in user.</li> <li>Adding a User to Chatroom: <code>POST</code> request to <code>/chatrooms/{chatroom_id}/add_user</code> with the username to add.</li> </ol> <p>Note: Make sure you have the correct user role for specific actions.</p>"},{"location":"celeryfastapi/lab3/#future-improvements","title":"Future Improvements","text":"<ul> <li>Implement chat functionality within chatrooms.</li> <li>Add more user roles and refine access controls.</li> <li>Enhance the chatroom interface and provide notifications for chatroom activity.</li> </ul>"},{"location":"celeryfastapi/lab4/","title":"Lab 4: Real-Time Chat Application","text":"<p>Lab 4 enhances the previous chatroom application by introducing real-time messaging capabilities, allowing users to send and receive messages instantaneously. This README provides all the necessary information to get started with the application, its usage, and the technical details.</p>"},{"location":"celeryfastapi/lab4/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Designs</li> <li>Usage</li> <li>API Endpoints</li> </ul>"},{"location":"celeryfastapi/lab4/#designs","title":"Designs","text":""},{"location":"celeryfastapi/lab4/#usage","title":"Usage","text":"<p>Once the application is running, you can interact with the chatroom functionalities. Use the following steps to get started:</p> <ol> <li>Create a Chatroom: Send a <code>POST</code> request to <code>/create/</code> with chatroom details.</li> <li>View Chatrooms: Send a <code>GET</code> request to <code>/</code> to view chatrooms for the logged-in user.</li> <li>Add User to Chatroom: Send a <code>POST</code> request to <code>/chatrooms/{chatroom_id}/add_user</code> with the username to add.</li> <li>Send a Message: Connect to the WebSocket endpoint and send a message payload.</li> </ol> <p>For detailed API usage and WebSocket interaction, refer to the API Endpoints section.</p>"},{"location":"celeryfastapi/lab4/#new-api-endpoints","title":"New API Endpoints","text":"<p>List your API endpoints here, for example:</p> <ul> <li><code>POST /create/</code> - Create a new chatroom.(lab 3)</li> <li><code>GET /</code> - Retrieve the list of chatrooms.(lab 3)</li> <li><code>POST /chatrooms/{chatroom_id}/add_user</code> - Add a user to a specific chatroom.(lab 3)</li> <li>WebSocket <code>chatroom/ws/</code> - WebSocket endpoint for real-time messaging.(new)</li> </ul>"},{"location":"celeryfastapi/lab5/","title":"Lab 5:  WebSocket With Celery","text":""},{"location":"celeryfastapi/lab5/#overview","title":"Overview","text":"<p>Our FastAPI application has been enhanced with a new WebSocket module, bringing real-time capabilities to our service. This update primarily revolves around providing live updates on task statuses.</p>"},{"location":"celeryfastapi/lab5/#how-it-works","title":"How It Works","text":""},{"location":"celeryfastapi/lab5/#background-perspective","title":"Background Perspective","text":"<p>In the backend, when a user is created or a task is initiated, our FastAPI app communicates with the database and Celery. Once a task is handed over to Celery, it is executed in the background. Upon completion, the task\u2019s post-run handler updates the task status which is then broadcasted through WebSocket to any subscribed clients.</p> <p>Key components involved: - FastAPI App: Handles user creation and task initiation. - Celery: Executes background tasks and notifies on completion. - WebSocket: Broadcasts real-time updates to clients.</p>"},{"location":"celeryfastapi/lab5/#user-perspective","title":"User Perspective","text":"<p>From the user\u2019s perspective, upon initiating a task (such as user creation or sending an email), they receive a task ID and can connect to a WebSocket endpoint to receive live updates about the task status. This connection keeps the user informed in real-time until the task is completed.</p> <p>The flow involves: - Client to FastAPI App: Requesting task initiation. - FastAPI App to Client: Providing task details. - Client to WebSocket: Subscribing for updates on the task. - WebSocket Broadcast: Notifying client of task status updates.</p>"},{"location":"celeryfastapi/lab5/#technical-implementation","title":"Technical Implementation","text":""},{"location":"celeryfastapi/lab5/#websocket-routes","title":"WebSocket Routes","text":"<p>The WebSocket functionality is facilitated through routes defined in <code>ws_router</code>. Key routes include: - <code>/form_ws/</code>: Displays a form for user interaction. - <code>/ws/task_status/{task_id}</code>: WebSocket endpoint for receiving task status updates.</p>"},{"location":"celeryfastapi/lab5/#frontend-integration","title":"Frontend Integration","text":"<p>The user interface interacts with these WebSocket endpoints. The frontend script handles form submission and WebSocket communication, providing a seamless experience for monitoring task progress.</p>"},{"location":"celeryfastapi/lab5/#testing-the-websocket-endpoint","title":"Testing the WebSocket Endpoint","text":""},{"location":"celeryfastapi/lab5/#endpoint-overview","title":"Endpoint Overview","text":"<p>The WebSocket endpoint at <code>http://localhost:8010/form_ws/</code> provides a user interface for submitting data and observing real-time updates about the processing status via WebSockets. This form is a part of a Celery-based task processing system.</p>"},{"location":"celeryfastapi/lab5/#how-to-use","title":"How to Use","text":"<ol> <li>Accessing the Endpoint:</li> <li> <p>Navigate to <code>http://localhost:8010/form_ws/</code> in your web browser. You will see a form with fields for email, username, and password.</p> </li> <li> <p>Submitting the Form:</p> </li> <li>Fill in the fields with appropriate data.</li> <li>Click the \u2018Submit\u2019 button.</li> <li> <p>Upon submission, the form data is sent to the server using a POST request to the <code>/users/</code> endpoint.</p> </li> <li> <p>WebSocket Interaction:</p> </li> <li>When the form is submitted, a WebSocket connection is established to <code>/ws/task_status/[task_id]</code>, where <code>[task_id]</code> is a unique identifier for the task generated by the server.</li> <li>The page displays real-time updates about the task status (e.g., \u2018Processing\u2026\u2019, \u2018Success\u2019, or any error messages) below the form.</li> <li> <p>Once the task reaches a final state (\u2018SUCCESS\u2019 or \u2018FAILURE\u2019), the WebSocket connection is closed.</p> </li> <li> <p>Front-End Behavior:</p> </li> <li>The form uses Bootstrap for styling and layout.</li> <li>JavaScript is utilized to handle form submission, WebSocket communication, and updating the UI based on the task\u2019s progress and status.</li> </ol>"},{"location":"img/celery/lab1/flow/","title":"Flow","text":"<p>sequenceDiagram     autonumber     Developer-&gt;&gt;FastAPI: Request to /divide-celery     FastAPI-&gt;&gt;CeleryTask: Invoke divide.delay(a,b)     Note over CeleryTask: Returns Task ID     CeleryTask-&gt;&gt;Redis: Push task     loop Check Task         Redis-&gt;&gt;CeleryTask: Notify of task     end     Note right of CeleryTask: Process task     CeleryTask-&gt;&gt;Redis: Save result     Developer-&gt;&gt;FastAPI: Request to /query-task-status     FastAPI-&gt;&gt;Redis: Fetch result     Redis\u2013&gt;&gt;FastAPI: Return result     FastAPI\u2013&gt;&gt;Developer: Respond with result     loop Poll for Tasks         Flower-&gt;&gt;Redis: Request all task details         Redis\u2013&gt;&gt;Flower: Return all tasks details     end     Developer-&gt;&gt;Flower: View Dashboard     Note over Flower: Shows tasks status</p>"},{"location":"kubernetes/","title":"Kubernetes","text":"<p>Mis anotaciones de kubernetes</p>"},{"location":"kubernetes/#preparacion-de-laboratorio-local-en-linux","title":"Preparacion de laboratorio local en linux","text":"<p>Herramientas necesarias:</p> <ul> <li>Kubectl <pre><code># download\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n# check checksum \ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\n# install \nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n# check installation\nkubectl version --client\n</code></pre></li> </ul> <ul> <li>Minikube <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n</code></pre></li> <li>Quemu Kvm <pre><code>sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager\nsudo systemctl is-active libvirtd\nsudo usermod -aG libvirt $USER\nsudo usermod -aG kvm $USER\n</code></pre></li> </ul>"},{"location":"kubernetes/#minilab-levantar-nginx-en-un-pod-en-minikube","title":"Minilab Levantar NGINX en un pod en MINIKUBE","text":"<p>En este laboratorio se levantara un pod con un servicio de nginx y se expondra el puerto a la maquina local.</p> <pre><code># Create a cluster\nminikube start --driver=kvm2\n# start the pod running nginx\nkubectl create deployment --image=nginx nginx-app\n# [OPTIONAL] add env to nginx-app\nkubectl set env deployment/nginx-app  DOMAIN=cluster\n# expose a port through with a service\nkubectl expose deployment nginx-app --port=80 --name=nginx-http\n# [TEST] access to service\ncurl 127.0.0.1:8010\n</code></pre>"},{"location":"kubernetes/#minilab-levantar-imagen-local-de-docker-en-kubernetes","title":"Minilab Levantar  imagen local de Docker en Kubernetes","text":"<p>En este laboratorio se creara una imagen docker de prueba  y se desplegara en un <code>pod</code>.</p> <ul> <li>Crear <code>Dockerfile</code>  que imprime en pantalla el mensaje <code>Hello World!</code>.</li> </ul> <pre><code>FROM busybox\nCMD [ \"echo\", \"Hello World!\"]\n</code></pre> <ul> <li>Crear la imagen Docker.</li> </ul> <pre><code>$ docker build . -t santos/hello-world\n    # Sending build context to Docker daemon  3.072kB\n    # Step 1/2 : FROM busybox\n    #  ---&gt; 62aedd01bd85\n    # Step 2/2 : CMD [ \"echo\", \"Hello World!\"]\n    #  ---&gt; Using cache\n    #  ---&gt; 79b4e6c1bd0d\n    # Successfully built 79b4e6c1bd0d\n    # Successfully tagged santos/hello-world:latest\n</code></pre> <ul> <li>Levantar un contenedor de prueba.</li> </ul> <pre><code>$ docker run santos/hello-world\n    # Hello World!\n</code></pre> <ul> <li>Crear el archivo de configuracion <code>helloworld.yml</code> para crear un job de kubernetes que corra el container  Docker. <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello-world\nspec:\n  template:\n    metadata:\n      name: hello-world-pod\n    spec:\n      containers:\n      - name: hello-world\n        image: santos/hello-world\n        imagePullPolicy: Never\n      restartPolicy: Never\n</code></pre> <code>restartPolicy</code> es <code>Never</code> debido que el container solo va a ejecutar el mensaje <code>Hello World!</code> y terminar, si se deja en <code>Always</code> se va a ejecutar infinitas veces.</li> </ul> <p><code>imagePullPolicy</code> es <code>Never</code> debido a que se quiere que busque la imagen localmente solamente, si no la encuentra no intentara bajarla del container registry.</p> <ul> <li> <p>Crear el job de kubernetes con este archivo de configuracion y <code>kubectl</code>. <pre><code>$ kubectl create -f helloworld.yml\n    # job.batch/hello-world created\n</code></pre></p> </li> <li> <p>Revisar si se ejecuto correctamente. <pre><code>$ kubectl get pods\n    # NAME              READY STATUS            RESTARTS AGE\n    # hello-world-r4g9g 0/1   ErrImageNeverPull 0        6s\n</code></pre></p> </li> </ul> <p>El error  <code>ErrImageNeverPull</code>  significa  que el nodo minikube usa su propio repositorio de Docker que no est\u00e1 conectado al registro de Docker en la m\u00e1quina local, por lo que, sin extraer, no sabe de d\u00f3nde obtener la imagen.</p> <p>Para solucionar esto, utilizo el comando <code>minikube docker-env</code> que genera las variables de entorno necesarias para apuntar el demonio local de Docker al registro interno de Docker de minikube:</p> <pre><code>$ minikube docker-env\n    # export DOCKER_TLS_VERIFY=\u201d1\"\n    # export DOCKER_HOST=\u201dtcp://172.17.0.2:2376\"\n    # export DOCKER_CERT_PATH=\u201d/home/user/.minikube/certs\u201d\n    # export MINIKUBE_ACTIVE_DOCKERD=\u201dminikube\u201d# To point your shell to minikube\u2019s docker-daemon, run:\n    ## eval $(minikube -p minikube docker-env)\n</code></pre> <ul> <li> <p>Aplicar las variables de <code>minikube</code> <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre></p> </li> <li> <p>Construyo la imagen de docker nuevamente, esta vez en el registro interno de <code>minukube</code>. <pre><code>$ docker build . -t santos/hello-world\n</code></pre></p> </li> <li> <p>Recrear el job otra vez. <pre><code>$ kubectl delete -f helloworld.yml\n$ kubectl create -f helloworld.yml\n</code></pre></p> </li> <li> <p>Revisar si se ejecuto correctamente. <pre><code>$ kubectl get pods\n    # NAME              READY STATUS            RESTARTS AGE\n    # hello-world-r4g9g 0/1   Completed         0        6s\n</code></pre></p> </li> <li> <p>Revisar los logs del pod. <pre><code>$ kubectl logs hello-world-f5hzz\n    # Hello World!\n</code></pre></p> </li> </ul>"},{"location":"kubernetes/labs1/","title":"Load Balancer y una API Flask","text":"<p>En este laboratorio se desplegara una aplicacion dockerizada hecha en <code>python</code> en <code>kubernetes</code>, agregando un <code>LoadBalancer</code>.</p> <ol> <li> <p>Crear el app de python <pre><code>#save as app.py \nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"hello word from flask!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', debug=True)\n</code></pre></p> </li> <li> <p>Crear el <code>Dockerfile</code> <pre><code>FROM python:3.7\nRUN mkdir /app\nWORKDIR /app/\nADD . /app/\nRUN pip install flask\nCMD [\"python\", \"/app/app.py\"]\n</code></pre></p> </li> <li> <p>Construir la imagen, pero antes apuntar al registry de <code>minukube</code>.</p> </li> </ol> <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre> <pre><code>$ docker build . -t santos/flask-hello-world\n</code></pre> <ol> <li>Crear archivo de configuracion <code>deployment.yml</code></li> </ol> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: flask-test-service\nspec:\n  selector:\n    app: flask-test-app # en nombre de la aplicacion que va a gestionar\n  ports:\n  - protocol: \"TCP\"\n    port: 6000\n    targetPort: 5000 # El puerto donde se expone  la aplicacion Flask\n  type: LoadBalancer\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-test-app\nspec:\n  selector:\n    matchLabels:\n      app: flask-test-app\n  replicas: 5  # \n  template:\n    metadata:\n      labels:\n        app: flask-test-app\n    spec:\n      containers:\n      - name: flask-test-app\n        image: santos/flask-hello-world\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 5000\n</code></pre> <ol> <li> <p>Aplicar el archivo de configuracion con <code>kubectl</code>. <pre><code>$ kubectl create -f deployment.yml\n    # service/flask-test-service created\n    # deployment.apps/flask-test-app created\n</code></pre></p> </li> <li> <p>Revisar si se ejecuto correctamente. <pre><code>$ kubectl get pods\n  # NAME              READY STATUS            RESTARTS AGE\n  # flask-test-app-c6b649857-2tgs5     1/1     Running            0                14s\n  # flask-test-app-c6b649857-g7f28     1/1     Running            0                14s\n  # flask-test-app-c6b649857-lh4nb     1/1     Running            0                14s\n  # flask-test-app-c6b649857-rsdjr     1/1     Running            0                14s\n  # flask-test-app-c6b649857-vmsj5     1/1     Running            0                14s\n\n$ kubectl get services\n  # NAME                  TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\n  # flask-test-service    LoadBalancer   10.102.69.135    &lt;pending&gt;     6000:31902/TCP   81s\n</code></pre></p> </li> <li> <p>Obtener la url de acceso al LoadBalancer. Como se esta usando <code>minkube</code> se utiliza el siguiente comando.</p> </li> </ol> <p><pre><code>$ minikube service example-service --url\n  # http://192.168.39.232:31902\n</code></pre> Si no se esta usando minikube se utiliza <code>kubectl describe</code></p> <pre><code>$ kubectl describe services flask-test-service\n  # Name:                     flask-test-service\n  # Namespace:                default\n  # Labels:                   &lt;none&gt;\n  # Annotations:              &lt;none&gt;\n  # Selector:                 app=flask-test-app\n  # Type:                     LoadBalancer\n  # IP Family Policy:         SingleStack\n  # IP Families:              IPv4\n  # IP:                       10.102.69.135\n  # IPs:                      10.102.69.135\n  # LoadBalancer Ingress:     192.168.39.232\n  # Port:                     &lt;unset&gt;  6000/TCP\n  # TargetPort:               5000/TCP\n  # NodePort:                 &lt;unset&gt;  31902/TCP\n  # Endpoints:                172.17.0.12:5000,172.17.0.3:5000,172.17.0.6:5000 + 2 more...\n  # Session Affinity:         None\n  # External Traffic Policy:  Cluster\n  # Events:                   &lt;none&gt;\n</code></pre> <p>En la salida del comando se puede ver informacion del <code>LoadBalancer</code>, el primero que necesitamos es <code>LoadBalancer Ingress</code>.</p> <ol> <li>Hacer un get al API: <pre><code>$ curl  http://192.168.39.232:31902\n  # hello word from flask!  \n</code></pre></li> </ol>"},{"location":"kubernetes/labs2/","title":"Compartir archivos entre  dos containers en un Pod","text":"<p>En este laboratorio se comunicaran dos <code>containers</code> que viven en un mismo <code>pod</code> usando <code>volumenes</code>.</p> <p></p> <ol> <li> <p>Crear el el <code>pod</code> con un <code>volumen</code> , y dos <code>containers</code> <pre><code># save as ./two-containers-and-volume.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n\n  restartPolicy: Never\n\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n\n  containers:\n\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"echo Hello from the debian container &gt; /pod-data/index.html\"]\n</code></pre> Como se puede ver se crea un <code>Pod</code> llamado <code>two-containers</code> y dentro de la especificacion se crea un <code>volumen</code> llamado <code>shared-data</code> , luego se crea un <code>container</code> llamado <code>nginx-container</code> montando el volumen en  <code>/usr/share/nginx/html</code>, y por ultimo se crea el <code>container</code> llamado <code>debian-container</code>, montando el volumen en  <code>/pod-data</code>. El segundo container ejecutara el siguiente comando y  terminara su ejecucion. <pre><code>$ echo Hello from the debian container &gt; /pod-data/index.html\n</code></pre> El segundo container modificara el archivo <code>index.html</code> de la carpeta principal del servidor <code>nginx</code> del segundo container, ya que este esta en el volumen compartido.</p> </li> <li> <p>Crear el <code>pod</code> usando <code>kubectl</code> <pre><code>$ kubectl apply -f ./two-containers-and-volume.yaml \n  # pod/two-containers created\n</code></pre></p> </li> <li> <p>Para ver la informacion detallada del <code>pod</code></p> </li> </ol> <p><pre><code>$ kubectl get pod two-containers --output=yaml\n</code></pre> output kubectl get pod two-containers<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/usr/share/nginx/html\",\"name\":\"shared-data\"}]},{\"args\":[\"-c\",\"echo Hello from the debian container \\u003e /pod-data/index.html\"],\"command\":[\"/bin/sh\"],\"image\":\"debian\",\"name\":\"debian-container\",\"volumeMounts\":[{\"mountPath\":\"/pod-data\",\"name\":\"shared-data\"}]}],\"restartPolicy\":\"Never\",\"volumes\":[{\"emptyDir\":{},\"name\":\"shared-data\"}]}}\n  creationTimestamp: \"2022-06-16T14:26:55Z\"\n  name: two-containers\n  namespace: default\n  resourceVersion: \"35653\"\n  uid: a2f25f74-e860-465d-b0e9-1fbdfa9d8a1a\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: Always\n    name: nginx-container\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-mvrgj\n      readOnly: true\n  - args:\n    - -c\n    - echo Hello from the debian container &gt; /pod-data/index.html\n    command:\n    - /bin/sh\n    image: debian\n    imagePullPolicy: Always\n    name: debian-container\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /pod-data\n      name: shared-data\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-mvrgj\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: minikube\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Never\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - emptyDir: {}\n    name: shared-data\n  - name: kube-api-access-mvrgj\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    status: \"True\"\n    type: Initialized\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    message: 'containers with unready status: [debian-container]'\n    reason: ContainersNotReady\n    status: \"False\"\n    type: Ready\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    message: 'containers with unready status: [debian-container]'\n    reason: ContainersNotReady\n    status: \"False\"\n    type: ContainersReady\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    status: \"True\"\n    type: PodScheduled\n  containerStatuses:\n  - containerID: docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5\n    image: debian:latest\n    imageID: docker-pullable://debian@sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510\n    lastState: {}\n    name: debian-container\n    ready: false # (1)\n    restartCount: 0\n    started: false\n    state:\n      terminated:\n        containerID: docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5\n        exitCode: 0\n        finishedAt: \"2022-06-16T14:27:00Z\"\n        reason: Completed # (2)\n        startedAt: \"2022-06-16T14:27:00Z\"\n  - containerID: docker://00c92009ff5b762f5a26552f1515addbe118403ab4efbb1e2d1244c698f0ccb2\n    image: nginx:latest\n    imageID: docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514\n    lastState: {}\n    name: nginx-container\n    ready: true\n    restartCount: 0\n    started: true\n    state:\n      running:\n        startedAt: \"2022-06-16T14:26:57Z\"\n  hostIP: 192.168.39.232\n  phase: Running # (3)\n  podIP: 172.17.0.16\n  podIPs:\n  - ip: 172.17.0.16\n  qosClass: BestEffort\n  startTime: \"2022-06-16T14:26:55Z\"\n</code></pre></p> <ol> <li> <p>Es <code>false</code>  debido a que el contenedor solo ejecuta un <code>echo</code> y termina su ejecucion.</p> </li> <li> <p>El <code>container</code> termino su ejecucion con <code>exito</code>.</p> </li> <li>El <code>container</code> nginx esta corriendo porque es un <code>servidor</code>.</li> </ol> <p>Se puede ver que container <code>debian-container</code> termino su ejecucion y <code>nginx-container</code> sigue corriendo.</p> <ol> <li>Entrar al contenedor <code>nginx-container</code> <pre><code>$ kubectl exec -it two-containers -c nginx-container -- /bin/bash\n</code></pre></li> <li> <p>Revisar el archivo <code>index.html</code> de la carpeta compartida <pre><code>root@two-containers:/# cat /usr/share/nginx/html/index.html \n  # Hello from the debian container\n</code></pre></p> </li> <li> <p>Revisar la respuesta del servidor de <code>nginx</code> <pre><code>root@two-containers:/# curl localhost\n  # Hello from the debian container\n</code></pre></p> </li> </ol> <pre><code>{\"data\":{\"attributes\":{\"resource_names_by_hash\":{\"7d74f4592a9fd6ff\":\"POST businessloki.loki.svc.cluster.local:3100/loki/api/v1/push\",\"5fc09ce97131f406\":\"POST dc.services.visualstudio.com/v2/track\",\"424ed4ae196ebbaf\":\"POST rt.services.visualstudio.com/QuickPulseService.svc/ping\"}},\"type\":\"resource_names_by_hash\"}}\n</code></pre>"},{"location":"kubernetes/labs3/","title":"Ping desde entre dos containers de un mismo POD","text":"<p>En este laboratorio se mostrara que se pueden alcanzar containers en un mismo pod. En este laboratorio se usara la imagen <code>santos/flask-hello-world</code> de laboratorios atras, y tambien usara una configuracion sencilla nginx.</p> <ol> <li> <p>Crear el archivo de configuracion de nginx <code>nginx-vol/default.conf</code> el el directorio que vamos a enlazar con un volumen al container. <pre><code>server {\n    listen       80;\n    listen  [::]:80;\n    server_name  localhost;\n\n    location /flask_app {\n        proxy_pass http://127.0.0.1:5000/;   \n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n}\n</code></pre> Esta pequena configuracion sencilla de <code>ngnx</code> crea un <code>proxy_pass</code> de <code>localhost/flask_app</code> a <code>localhost:5000</code>, este es el puerto de escucha de la aplicacion de <code>flask</code> que se usara en este laboratorio.</p> </li> <li> <p>Recordar que kubernetes no tiene comunicacion directa con nuestra maquina, tiene comunicacion directa con la maquina virtual que monta minikube, como se quiere montar un directorio de nuestra maquina como un volumen, primero se debe montar en la maquina virtual de minikube. <pre><code>minikube mount nginx-vol/:/mynginx/\n# \ud83d\udcc1  Mounting host path myvol/ into VM as /mynginx/ ...\n#     \u25aa Mount type:   \n#     \u25aa User ID:      docker\n#     \u25aa Group ID:     docker\n#     \u25aa Version:      9p2000.L\n#     \u25aa Message Size: 262144\n#     \u25aa Options:      map[]\n#     \u25aa Bind Address: 192.168.39.1:40637\n# \ud83d\ude80  Userspace file server: ufs starting\n# \u2705  Successfully mounted myvol/ to /mynginx/\n\n# \ud83d\udccc  NOTE: This process must stay alive for the mount to be accessible ...\n</code></pre> El volumen estara montado mientras no terminemos este proceso. Ahora existe una carpeta llamada <code>/mynginx</code> en la <code>VM</code>.</p> </li> <li> <p>Yaml de creacion del <code>Pod</code> <pre><code># save as ./spec.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers-ping\nspec:\n\n  restartPolicy: Never\n  volumes:\n  - name: shared-data\n    hostPath:\n      path: /mynginx/\n      type: Directory\n\n  containers:\n\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /etc/nginx/conf.d\n  - name: flask-test-app\n    image: santos/flask-hello-world\n    imagePullPolicy: Never\n    # ports:\n    # - containerPort: 5000\n</code></pre></p> </li> <li> <p>Crear el <code>pod</code> usando <code>kubectl</code> <pre><code>$ kubectl apply -f ./spec.yaml \n  # pod/two-containers-ping created\n</code></pre></p> </li> <li> <p>Revisar el estado del nuevo <code>pod</code> <pre><code>kubectl get pod two-containers-ping --output=yaml\n</code></pre></p> </li> </ol> <p>output kubectl get pod<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers-ping\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/conf.d\",\"name\":\"shared-data\"}]},{\"image\":\"santos/flask-hello-world\",\"imagePullPolicy\":\"Never\",\"name\":\"flask-test-app\"}],\"restartPolicy\":\"Never\",\"volumes\":[{\"hostPath\":{\"path\":\"/mynginx/\",\"type\":\"Directory\"},\"name\":\"shared-data\"}]}}\n  creationTimestamp: \"2022-06-17T14:38:29Z\"\n  name: two-containers-ping\n  namespace: default\n  resourceVersion: \"52327\"\n  uid: 6dbb4caa-87b3-498a-8c4a-a7aeb76842af\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: Always\n    name: nginx-container\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /etc/nginx/conf.d\n      name: shared-data\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-8vpjq\n      readOnly: true\n  - image: santos/flask-hello-world\n    imagePullPolicy: Never\n    name: flask-test-app\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-8vpjq\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: minikube\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Never\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - hostPath:\n      path: /mynginx/\n      type: Directory\n    name: shared-data\n  - name: kube-api-access-8vpjq\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:29Z\"\n    status: \"True\"\n    type: Initialized\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:31Z\"\n    status: \"True\"\n    type: Ready\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:31Z\"\n    status: \"True\"\n    type: ContainersReady\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:29Z\"\n    status: \"True\"\n    type: PodScheduled\n  containerStatuses:\n  - containerID: docker://0b53b339b7c06b7475739cbde17795ce24f71bc8d7d2e41da07d220162b6475d\n    image: santos/flask-hello-world:latest\n    imageID: docker://sha256:105bc09b0a7e87f7344040742f1fa011e00e7a36bac1f1cabc8a5303ceb1db30\n    lastState: {}\n    name: flask-test-app\n    ready: true\n    restartCount: 0\n    started: true\n    state:\n      running:\n        startedAt: \"2022-06-17T14:38:31Z\"\n  - containerID: docker://8175ac7cd60060159888a723b7a36a93486a38e9b6cd4379d7e86fbb4647375d\n    image: nginx:latest\n    imageID: docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514\n    lastState: {}\n    name: nginx-container\n    ready: true\n    restartCount: 0\n    started: true\n    state:\n      running:\n        startedAt: \"2022-06-17T14:38:31Z\"\n  hostIP: 192.168.39.232\n  phase: Running\n  podIP: 172.17.0.16\n  podIPs:\n  - ip: 172.17.0.16\n  qosClass: BestEffort\n  startTime: \"2022-06-17T14:38:29Z\"\n</code></pre> Como se puede ver los dos containers estan <code>Ready</code></p> <ol> <li> <p>Entrar al container   <code>nginx-container</code> <pre><code>$ kubectl exec -it two-containers-ping -c nginx-container -- /bin/bash\n  # root@two-containers-ping:/\n</code></pre></p> </li> <li> <p>Hacer un get a <code>http://localhost/flask_app/</code> <pre><code>$root@two-containers-ping:  curl http://localhost/flask_app\n  # hello word from flask!\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/labs3/#conclusion","title":"Conclusion","text":"<p>Containers within a pod share an IP address and port space, and can find each other via localhost s</p>"},{"location":"kubernetes/labs4/","title":"Comunicacion de dos PODS con un service NodePort","text":"<p>En este laboratorio se creara un servicio NodePort para que un pod pueda alcanzar a otro usando  un <code>SVCHOST</code>. </p> <ol> <li> <p>Para este laboratorio se creo una apliacion en flask que se conecta a una base de datos mongo, esta app tiene dos rutas, enviar mensaje, leer mensajes, los mensajes se guardan en la base de datos. <pre><code>#save as app.py \nfrom flask import Flask\nfrom flask import jsonify\nfrom pymongo import MongoClient\napp = Flask(__name__) \n# se usa el host del Servicio NodePort\nclient = MongoClient(\"mongo.default.svc.cluster.local\",27017)\ndb = client.test_db\n@app.route(\"/\")\ndef hello():\n    return \"works\"\n\n@app.route(\"/send_message/&lt;sender&gt;/&lt;to&gt;/&lt;message&gt;\")\ndef send_message(sender,to,message):\n    return str(db.messages.insert_one({'to':to,'message':message,'from':sender}).inserted_id)\n@app.route(\"/get_messages/&lt;to&gt;\")\ndef get_messages(to):\n    messages =  list(db.messages.find({\"to\":to},{\"_id\":0}))\n\n    if messages:\n        db.messages.delete_many({\"to\":to})   \n    print(messages)\n    return jsonify(messages     )\n</code></pre></p> </li> <li> <p>Escribir el <code>Dockerfile</code> <pre><code>FROM python:3.7\nRUN mkdir /app\nWORKDIR /app/\nRUN pip install flask==2.0.0 pymongo\n\nADD . /app/\n\nENV FLASK_APP=app.py\n\nENTRYPOINT [ \"flask\"]\nCMD [ \"run\", \"--host\", \"0.0.0.0\",\"-p\", \"5000\" ]\n</code></pre></p> </li> <li> <p>Crear la imagen <code>santos/flask-messages-app-with-mongo</code> a partir del <code>Dockerfile</code>, apuntar al registry de <code>minikube</code>.</p> </li> </ol> <p><pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre> <pre><code>$ docker build . -t santos/flask-messages-app-with-mongo\n</code></pre></p> <ol> <li> <p>Yaml de creacion de los dos <code>pods</code> usando <code>Deployments</code> <pre><code># save as ./deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-messages\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: flask-messages\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: flask-messages\n    spec:\n      containers:\n      - name: flask-messages\n        image:  santos/flask-messages-app-with-mongo\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 5000\n\n---\n# Deploy de la db\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image:  mongo\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 27017\n</code></pre></p> </li> <li> <p>Ejecutar el <code>deploment.yaml</code> <pre><code>$ kubectl apply -f ./deployment.yaml \n# deployment.apps/flask-messages created\n# deployment.apps/mongo created\n</code></pre></p> </li> <li> <p>Revisar el estado del cluster <pre><code>$ kubectl get pods\n\n  # flask-messages-b54fdfbb-2kkrl   1/1     Running   0          40m\n  # mongo-7f4df74f64-mdkgm          1/1     Running   0          40m\n\n$ kubectl get deployments\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\nflask-messages   1/1     1            1           41m\nmongo            1/1     1            1           41m\n</code></pre></p> </li> <li> <p>Yaml de creacion de servicios, uno para que la aplicacion pueda acceder a la db con un <code>hostname</code> y otra para poder acceder a la aplicacion desde la maquina local.</p> </li> </ol> <pre><code>#save as ./services.yaml\n\n\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: mongo\n  namespace: default\nspec:\n  ports:\n  - port: 27017\n    protocol: TCP\n    targetPort: 27017\n  selector:\n    app: mongo\n  type: NodePort\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: flask-messages\n  namespace: default\nspec:\n  ports:\n  - port: 5000\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: flask-messages\n  type: NodePort\n</code></pre> <ol> <li> <p>Crear los servicios <code>services.yaml</code> <pre><code>$ kubectl apply -f ./services.yaml \n  # service/mongo created\n  # service/flask-messages created\n</code></pre></p> </li> <li> <p>Revisar el estado del cluster</p> </li> </ol> <p><pre><code>$ kubectl get services\n  # NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n  # flask-messages   NodePort    10.109.12.181   &lt;none&gt;        5000:31377/TCP    46m\n  # kubernetes       ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP           96m\n  # mongo            NodePort    10.97.20.81     &lt;none&gt;        27017:31644/TCP   46m\n</code></pre> 7. Recordar  que kubernetes esta corriendo en una maquina virtual, por ende para poder acceder al servicio  <code>flask-messages</code> se debe obtener la url de acceso usando minikube <pre><code>$ minikube service flask-messages  --url \n  #  http://192.168.39.95:31377\n</code></pre></p> <ol> <li>Usar la aplicacion </li> </ol> <p><pre><code># Enviar mensajes\ncurl http://192.168.39.95:31377/new_message/santos/juan/holajuan\n# 62ae84f9ab010d71976d6c2a\ncurl http://192.168.39.95:31377/new_message/santos/juan/estoyenviandounmensajedesdekubernetes\n# 62ae8531ab010d71976d6c2b\n</code></pre> Leer mensajes <pre><code>$ curl http://192.168.39.95:31377/get_messages/juan\n</code></pre> <pre><code>[\n    {\n        \"from\": \"santos\",\n        \"message\": \"holajuan\",\n        \"to\": \"juan\"\n    },\n    {\n        \"from\": \"santos\",\n        \"message\": \"estoyenviandounmensajedesdekubernetes\",\n        \"to\": \"juan\"\n    }\n]\n</code></pre></p>"},{"location":"kubernetes/labs5/","title":"Docker Coins en Kubernetes","text":"<p>En este laboratorio montara la conocida  aplicacion <code>Docker Coins</code> en Kubernetes, especificamente se clono este repositorio <code>Docker Coins</code></p> <p></p> <p>Cada microservicio(<code>worker</code>,<code>rng</code>,<code>webui</code>,<code>hasher</code>) y redis se montara en <code>Pods</code> diferentes. 1. Clonar el repo: <pre><code>git clone https://github.com/platzi/curso-kubernetes\n# dockercoins projects is in dockercoins folder\n</code></pre> 2. Como se ve en el diagrama de arquitectura,  <code>worker</code> y  <code>webui</code> son los unicos microservicios que acceden a otros, se deben modificar las urls de acceso a los otros microservicios, para colocar las urls <code>svc</code> de kubernetes, ya que se usaran servicios <code>NodePort</code> para poder acceder a los microservicios, con los nombres: <code>redis</code>,<code>rng</code>,<code>hasher</code> respectivamente.</p> <p><pre><code>### dockercoins/webui/webui.js line 5\n- var client = redis.createClient(6379, 'redis');\n+ var client = redis.createClient(6379, 'redis.default.svc.cluster.local');\n</code></pre> <pre><code>### dockercoins/worker/worker.py line 17\n- redis = Redis(\"redis\")\n+ redis = Redis(\"redis.default.svc.cluster.local\")\n</code></pre> <pre><code>### dockercoins/worker/worker.py line 21\n- r = requests.get(\"http://rng/32\")\n+ r = requests.get(\"http://rng.default.svc.cluster.local/32\")\n</code></pre> <pre><code>### dockercoins/worker/worker.py line 26,27,28\n- r = requests.post(\"http://hasher/\",\n-                      data=data,\n-                      headers={\"Content-Type\": \"application/octet-stream\"})\n+ r = requests.post(\"http://hasher.default.svc.cluster.local/\",\n+                      data=data,\n+                      headers={\"Content-Type\": \"application/octet-stream\"})\n</code></pre></p> <p>Notar que:</p> <ul> <li>En nodejs hay que poner la url sin <code>http://</code>.</li> <li>En python si hay que colocarlo en las urls que se consumen con la libreria <code>requests</code>.</li> <li>La url de  <code>redis</code> no lleva el protocolo:  <code>redis.default.svc.cluster.local</code>. </li> </ul> <ol> <li>Crear las imagenes de docker, por facilidad se usara docker-compose para generar todas las imagenes con un comando. Antes apuntar al registry de <code>minikube</code> <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre> <pre><code>## TODO ignore redis build\n$ docker-compose  -f dockercoins/docker-compose.yml  build\n</code></pre></li> <li>Renombrar las imagenes</li> </ol> <p><pre><code>docker tag dockercoins_worker  santos/docker-coins-worker-app\ndocker tag dockercoins_hasher santos/docker-coins-hasher-app\ndocker tag dockercoins_rng:latest   santos/docker-coins-rng-app\ndocker tag dockercoins_webui santos/docker-coins-web-ui-app\n</code></pre> 5. Crear los archivos de depliegue de <code>kubernetes</code> config.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-configmap\n  namespace: default\ndata:\n  redis.conf: |\n    protected-mode no\n    maxmemory 32mb\n    maxmemory-policy allkeys-lru\n# cat /usr/local/etc/redis/redis.conf \n\n---\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-sysctl\n  namespace: default\ndata:\n  sysctl.conf: |\n    net.core.somaxconn=511\n    vm.overcommit_memory=1\n    net.core.somaxconn=4096\n</code></pre> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: web-ui\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: web-ui\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: web-ui\n        image:  santos/docker-coins-web-ui-app\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: worker\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: worker\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: worker\n        image:  santos/docker-coins-worker-app\n        imagePullPolicy: Never\n        # ports:\n        # - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: redis\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: redis\n        image:  redis\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        volumeMounts:\n        - mountPath: /usr/local/etc/redis/redis.conf\n          name: redis-configmap\n          subPath: redis.conf\n\n        - mountPath: /etc/sysctl.conf\n          name: redis-sysctl\n          subPath: sysctl.conf\n        command: [\"redis-server\", \"/usr/local/etc/redis/redis.conf\"]\n      volumes:\n      - name: redis-configmap\n        configMap:\n          name: redis-configmap\n      - name: redis-sysctl\n        configMap:\n          name: redis-sysctl\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hasher\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: hasher\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: hasher\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: hasher\n        image:  santos/docker-coins-hasher-app\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rng\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: rng\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: rng\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: rng\n        image:  santos/docker-coins-rng-app\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n</code></pre> services.yaml<pre><code>#save as ./services.yaml\n\n\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: web-ui\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: web-ui\n  type: NodePort\n---\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: redis\n  namespace: default\nspec:\n  ports:\n  - port: 6379\n    # protocol: TCP\n    targetPort: 6379\n  selector:\n    app: redis\n  type: NodePort\n---\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: hasher\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: hasher\n  type: NodePort\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name:  rng\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app:  rng\n  type: NodePort\n</code></pre> Notar que en los deployments hay una etiqueta llamada <code>masterapp</code> con el valor dockercoins, la cual servira para hacer operaciones sobre todos los deployments.  6. Applicar los archivos de configuracion <pre><code>$ kubectl apply -f config.yaml &amp;&amp; kubectl apply -f services.yaml \\\n &amp;&amp; kubectl apply -f deployment.yaml\n  # configmap/redis-configmap created\n  # configmap/redis-sysctl created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n  # deployment.apps/web-ui created\n  # deployment.apps/worker created\n  # deployment.apps/redis created\n  # deployment.apps/hasher created\n  # deployment.apps/rng created\n  # pod/dnsutils created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n</code></pre> 7. Ver los pods <pre><code>NAME                            READY   STATUS    RESTARTS      AGE\nhasher-6fd7cfd7bf-8v86t         1/1     Running   0             7m51s\nredis-594c4d767f-8xfrd          1/1     Running   0             7m51s\nrng-648c7c46c7-qrn2m            1/1     Running   0             7m51s\nweb-ui-57b7674785-7cfj2         1/1     Running   0             7m51s\nworker-67ff756f89-wvmt4         1/1     Running   0             7m51s\n</code></pre> 8. Ver los logs de todos los deployments</p> <p><pre><code>$ kubectl logs --selector masterapp=dockercoins\n</code></pre> <pre><code>1:C 21 Jun 2022 20:47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 21 Jun 2022 20:47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 21 Jun 2022 20:47:44.842 # Configuration loaded\n1:M 21 Jun 2022 20:47:44.842 * monotonic clock: POSIX clock_gettime\n1:M 21 Jun 2022 20:47:44.843 * Running mode=standalone, port=6379.\n1:M 21 Jun 2022 20:47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Jun 2022 20:47:44.843 # Server initialized\n1:M 21 Jun 2022 20:47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0c7d1044...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0fe2be6c...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\n\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1008\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1006\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:51 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n\n\nWEBUI running on port 80\n</code></pre> 9. Exponer el servicio <code>NodePort</code> de <code>web-ui</code> con minikube  <pre><code>$ minikube service web-ui --url\n  # http://192.168.39.95:31108\n</code></pre> 10. Abrir el link en el navegador</p> <p></p>"},{"location":"kubernetes/labs5/#posibles-errores","title":"Posibles errores","text":"<ol> <li>Resolucion de <code>hostnames</code> <code>svc</code> en el microservicio <code>worker</code> o <code>webui</code>, puede ser porque se creo el servicio <code>NodePort</code> de los otros microservicios despues de que se ejecutaran los deplyments,  es decir no existian esos hostnames, para esto se puede reiniciar el deployment fallido.</li> </ol> <p><pre><code>$ kubectl rollout restart deployment worker\n  # deployment.apps/worker restarted\n\n$ kubectl rollout restart deployment web-ui\n  # deployment.apps/web-ui restarted\n</code></pre> 2. <code>Connection refused</code> o <code>ENOTFOUND</code>, recordar que:</p> <ul> <li>En nodejs hay que poner la url sin <code>http://</code>.</li> <li>En python si hay que colocar el protocolo en  las urls que se consumen con la libreria <code>requests</code>.</li> <li>La url de  <code>redis</code> no lleva el protocolo:  <code>redis.default.svc.cluster.local</code>.</li> <li><code>ImageNeverPull</code>, no se apunto al registry de <code>minikube</code>: <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre></li> </ul>"},{"location":"kubernetes/labs6/","title":"Azure Kubernetes Service","text":"<p>En este lab se mostrara como levantar un <code>container registry</code> y un <code>cluster</code> en Azure, y luego se desplegara la aplicacion <code>Docker Coins</code>.</p>"},{"location":"kubernetes/labs6/#crear-los-recursos-en-azure","title":"Crear los recursos en Azure","text":"<ol> <li>Crear el <code>Container Registry</code> para subir las imagenes de docker:</li> </ol> <p><pre><code>az acr create --resource-group solok8s \\\n  --name solok8sregistry --sku Basic\n</code></pre> 2. Crear un Cluster <pre><code>az aks create --resource-group solok8s --name dockercoins --node-count 1 --generate-ssh-keys\n</code></pre> 3. Realcionar el Cluster al ACR Creado en el paso 1 <pre><code>az aks update -n dockercoins  -g solok8s  --attach-acr solok8sregistry\n</code></pre></p> <p>Con estos comandos ya se tiene un cluster y un acr para empezar a desplegar aplicaciones Ahora bien para poder manipular estos dos desde la terminar se deben ejecutar los siguientes comandos </p>"},{"location":"kubernetes/labs6/#acceder-a-los-recursos-desde-la-terminal","title":"Acceder a los recursos desde la terminal","text":"<ol> <li>Login en el <code>ACR</code>, para poder publicar imagenes <pre><code>az acr  login  --name  solok8sregistry\n</code></pre></li> <li> <p>Obtener el nombre   dominio del nuevo container registry. <pre><code>az acr show --name solok8sregistry --query loginServer --output table\n# Result\n# --------------------------\n# solok8sregistry.azurecr.io\n</code></pre> Para subir una imagen al registry se debe nombrar de la siguiente forma <code>&lt;registry&gt;/&lt;name&gt;:&lt;tag&gt;</code>, en este caso a modo de ejemplo <code>solok8sregistry.azurecr.io/regis:latest</code></p> </li> <li> <p>Obterner las credenciales de acceso al Cluster <pre><code>az aks get-credentials --resource-group solok8s --name dockercoins\n</code></pre> Ya se puede acceder al cluster con kubectl <pre><code>kubectl get pods\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/labs6/#subir-docker-coins-al-nuevo-cluster","title":"Subir Docker Coins al Nuevo Cluster","text":"<p>Se deben seguir los mismos pasos que con minikube, pero previamente estando autenticado en el <code>ACR</code> y el <code>Cluster</code>. 1. Renombrar las imagenes que se quieren subir al nuevo <code>ACR</code></p> <p><pre><code>docker tag dockercoins_worker  solok8sregistry.azurecr.io/docker-coins-worker-app\ndocker tag dockercoins_hasher solok8sregistry.azurecr.io/docker-coins-hasher-app\ndocker tag dockercoins_rng:latest   solok8sregistry.azurecr.io/docker-coins-rng-app\ndocker tag dockercoins_webui solok8sregistry.azurecr.io/docker-coins-web-ui-app\n</code></pre> 2. publicar las imagenes en el <code>ACR</code></p> <p><pre><code>docker push solok8sregistry.azurecr.io/docker-coins-worker-app\ndocker push solok8sregistry.azurecr.io/docker-coins-hasher-app\ndocker push solok8sregistry.azurecr.io/docker-coins-rng-app\ndocker push solok8sregistry.azurecr.io/docker-coins-web-ui-app\n</code></pre> 3. Se pueden usar los mismos archivos <code>yaml</code> que se usaron con minikube. Applicar los archivos de configuracion <pre><code>$ kubectl apply -f config.yaml &amp;&amp; kubectl apply -f services.yaml \\\n &amp;&amp; kubectl apply -f deployment.yaml\n  # configmap/redis-configmap created\n  # configmap/redis-sysctl created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n  # deployment.apps/web-ui created\n  # deployment.apps/worker created\n  # deployment.apps/redis created\n  # deployment.apps/hasher created\n  # deployment.apps/rng created\n  # pod/dnsutils created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n</code></pre> 4. Ver los pods <pre><code>NAME                            READY   STATUS    RESTARTS      AGE\nhasher-6fd7cfd7bf-8v86t         1/1     Running   0             7m51s\nredis-594c4d767f-8xfrd          1/1     Running   0             7m51s\nrng-648c7c46c7-qrn2m            1/1     Running   0             7m51s\nweb-ui-57b7674785-7cfj2         1/1     Running   0             7m51s\nworker-67ff756f89-wvmt4         1/1     Running   0             7m51s\n</code></pre> 5. Ver los logs de todos los deployments</p> <p><pre><code>$ kubectl logs --selector masterapp=dockercoins\n</code></pre> <pre><code>1:C 21 Jun 2022 20:47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 21 Jun 2022 20:47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 21 Jun 2022 20:47:44.842 # Configuration loaded\n1:M 21 Jun 2022 20:47:44.842 * monotonic clock: POSIX clock_gettime\n1:M 21 Jun 2022 20:47:44.843 * Running mode=standalone, port=6379.\n1:M 21 Jun 2022 20:47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Jun 2022 20:47:44.843 # Server initialized\n1:M 21 Jun 2022 20:47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0c7d1044...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0fe2be6c...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\n\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1008\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1006\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:51 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n\n\nWEBUI running on port 80\n</code></pre> 6. Hacer un portforward al service web-ui en el puerto <code>80</code></p> <p>kubectl port-forward svc/web-ui 8011:80 7. Abrir en el navegador la url <code>localhost:8011</code></p> <p></p>"},{"location":"kubernetes/labs7/","title":"Deploy Cronjob to fetch siigo costs","text":"<p>En este lab se mostrara como desplegar <code>Siigo Costs</code> en minikube y configurar dos <code>cronjobs</code> para generar los costos de infraestructura mensualmente.</p>"},{"location":"kubernetes/labs7/#dockerizar-la-aplicacion","title":"Dockerizar la aplicacion","text":"<p>Siigo costs es una aplicacion creada en Python, no usa frameworks por ende la imagen de Docker es sencilla</p> <p><pre><code>FROM python:slim-buster\n\nENV PYTHONUNBUFFERED 1\nENV PYTHONDONTWRITEBYTECODE 1\n\nWORKDIR /src\n\nCOPY requirements.txt /src/requirements.txt\n\nRUN pip install --upgrade pip\nRUN pip install --no-cache-dir -r /src/requirements.txt\nCOPY . /src/\n</code></pre> Agregar assets al archivo dockerignore para que no se  agregue la carpeta al contexto del build</p> <pre><code>$ echo 'assets/ &gt;&gt; .dockerignore'\n</code></pre> <p>Crear la imagen Nota: Recureda primero apuntar al registry de Minukube</p> <pre><code>eval $(minikube -p minikube docker-env)\n</code></pre> <p>Ahora si \u2026</p> <pre><code>$ docker build . -t  santos/siigo-costs\n</code></pre>"},{"location":"kubernetes/labs7/#crear-el-cronjob","title":"Crear el Cronjob","text":"<p>Se creara directamente un cronjob que baje la imagen creada, y se ejecute el primero de cada mes. El script que ejecuta el cron es determinar el mes y el anio, y lanzar la app siigo costs.</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: FerchTempCosts\nspec:\n  schedule: \"*12 11 1 * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: siigo-costs-cron\n            image: santos/siigo-costs\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - month=$(date +'%m');year=$(date +'%Y'); python manage.py fetch_costs $month $year $month $year 1 0;\n            # - python manage.py fetch_costs $month $year $month $year 1 0;\n          restartPolicy: OnFailure\n</code></pre> <p>Hace falta montar assets con un volumen al container, para eso primero se debe enlazar el volumen a la vm de minikube:</p> <p><pre><code>~costs_fetcher_docker$ minikube mount $PWD/assets:/siigo-costs/assets\n\ud83d\udcc1  Mounting host path /home/informatica/dev1/prod_siigo_billing/costs_fetcher_docker/assets into VM as /siigo-costs/assets ...\n    \u25aa Mount type:   \n    \u25aa User ID:      docker\n    \u25aa Group ID:     docker\n    \u25aa Version:      9p2000.L\n    \u25aa Message Size: 262144\n    \u25aa Options:      map[]\n    \u25aa Bind Address: 192.168.39.1:44333\n\ud83d\ude80  Userspace file server: ufs starting\n\u2705  Successfully mounted /home/informatica/dev1/prod_siigo_billing/costs_fetcher_docker/assets to /siigo-costs/assets\n</code></pre> Ahora modificaremos el yaml agregando el volumen</p> <p><pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: FerchTempCosts\nspec:\n  schedule: \"*12 11 1 * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: siigo-costs-cron\n            image: santos/siigo-costs\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - ls /src/assets/;sleep 20;month=$(date +'%m');year=$(date +'%Y'); python manage.py fetch_costs $month $year $month $year 1 0;\n\n            volumeMounts:\n              - name: assets\n                mountPath: /src/assets\n          restartPolicy: OnFailure\n          volumes:\n          - name: assets\n            hostPath:\n              path: /siigo-costs/assets\n              type: Directory\n</code></pre> Por ultimo aplicaremos el cronjob</p> <pre><code>$ kubect apply -f cronjob.yaml\n</code></pre>"},{"location":"kubernetes/notes/","title":"Notes","text":"<p>resources in request should be the same to </p>"},{"location":"labs_kubernetes/azure/labs1/","title":"1. Azure Kubernetes Service","text":"<p>En este lab se mostrara como levantar un <code>container registry</code> y un <code>cluster</code> en Azure, y luego se desplegara la aplicacion <code>Docker Coins</code>.</p>"},{"location":"labs_kubernetes/azure/labs1/#crear-los-recursos-en-azure","title":"Crear los recursos en Azure","text":"<ol> <li>Crear el <code>Container Registry</code> para subir las imagenes de docker:</li> </ol> <p><pre><code>az acr create --resource-group solok8s \\\n  --name solok8sregistry --sku Basic\n</code></pre> 2. Crear un Cluster <pre><code>az aks create --resource-group solok8s --name dockercoins --node-count 1 --generate-ssh-keys\n</code></pre> 3. Realcionar el Cluster al ACR Creado en el paso 1 <pre><code>az aks update -n dockercoins  -g solok8s  --attach-acr solok8sregistry\n</code></pre></p> <p>Con estos comandos ya se tiene un cluster y un acr para empezar a desplegar aplicaciones Ahora bien para poder manipular estos dos desde la terminar se deben ejecutar los siguientes comandos </p>"},{"location":"labs_kubernetes/azure/labs1/#acceder-a-los-recursos-desde-la-terminal","title":"Acceder a los recursos desde la terminal","text":"<ol> <li>Login en el <code>ACR</code>, para poder publicar imagenes <pre><code>az acr  login  --name  solok8sregistry\n</code></pre></li> <li> <p>Obtener el nombre   dominio del nuevo container registry. <pre><code>az acr show --name solok8sregistry --query loginServer --output table\n# Result\n# --------------------------\n# solok8sregistry.azurecr.io\n</code></pre> Para subir una imagen al registry se debe nombrar de la siguiente forma <code>&lt;registry&gt;/&lt;name&gt;:&lt;tag&gt;</code>, en este caso a modo de ejemplo <code>solok8sregistry.azurecr.io/regis:latest</code></p> </li> <li> <p>Obterner las credenciales de acceso al Cluster <pre><code>az aks get-credentials --resource-group solok8s --name dockercoins\n</code></pre> Ya se puede acceder al cluster con kubectl <pre><code>kubectl get pods\n</code></pre></p> </li> </ol>"},{"location":"labs_kubernetes/azure/labs1/#subir-docker-coins-al-nuevo-cluster","title":"Subir Docker Coins al Nuevo Cluster","text":"<p>Se deben seguir los mismos pasos que con minikube, pero previamente estando autenticado en el <code>ACR</code> y el <code>Cluster</code>. 1. Renombrar las imagenes que se quieren subir al nuevo <code>ACR</code></p> <p><pre><code>docker tag dockercoins_worker  solok8sregistry.azurecr.io/docker-coins-worker-app\ndocker tag dockercoins_hasher solok8sregistry.azurecr.io/docker-coins-hasher-app\ndocker tag dockercoins_rng:latest   solok8sregistry.azurecr.io/docker-coins-rng-app\ndocker tag dockercoins_webui solok8sregistry.azurecr.io/docker-coins-web-ui-app\n</code></pre> 2. publicar las imagenes en el <code>ACR</code></p> <p><pre><code>docker push solok8sregistry.azurecr.io/docker-coins-worker-app\ndocker push solok8sregistry.azurecr.io/docker-coins-hasher-app\ndocker push solok8sregistry.azurecr.io/docker-coins-rng-app\ndocker push solok8sregistry.azurecr.io/docker-coins-web-ui-app\n</code></pre> 3. Se pueden usar los mismos archivos <code>yaml</code> que se usaron con minikube. Applicar los archivos de configuracion <pre><code>$ kubectl apply -f config.yaml &amp;&amp; kubectl apply -f services.yaml \\\n &amp;&amp; kubectl apply -f deployment.yaml\n  # configmap/redis-configmap created\n  # configmap/redis-sysctl created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n  # deployment.apps/web-ui created\n  # deployment.apps/worker created\n  # deployment.apps/redis created\n  # deployment.apps/hasher created\n  # deployment.apps/rng created\n  # pod/dnsutils created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n</code></pre> 4. Ver los pods <pre><code>NAME                            READY   STATUS    RESTARTS      AGE\nhasher-6fd7cfd7bf-8v86t         1/1     Running   0             7m51s\nredis-594c4d767f-8xfrd          1/1     Running   0             7m51s\nrng-648c7c46c7-qrn2m            1/1     Running   0             7m51s\nweb-ui-57b7674785-7cfj2         1/1     Running   0             7m51s\nworker-67ff756f89-wvmt4         1/1     Running   0             7m51s\n</code></pre> 5. Ver los logs de todos los deployments</p> <p><pre><code>$ kubectl logs --selector masterapp=dockercoins\n</code></pre> <pre><code>1:C 21 Jun 2022 20:47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 21 Jun 2022 20:47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 21 Jun 2022 20:47:44.842 # Configuration loaded\n1:M 21 Jun 2022 20:47:44.842 * monotonic clock: POSIX clock_gettime\n1:M 21 Jun 2022 20:47:44.843 * Running mode=standalone, port=6379.\n1:M 21 Jun 2022 20:47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Jun 2022 20:47:44.843 # Server initialized\n1:M 21 Jun 2022 20:47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0c7d1044...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0fe2be6c...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\n\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1008\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1006\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:51 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n\n\nWEBUI running on port 80\n</code></pre> 6. Hacer un portforward al service web-ui en el puerto <code>80</code></p> <p>kubectl port-forward svc/web-ui 8011:80 7. Abrir en el navegador la url <code>localhost:8011</code></p> <p></p>"},{"location":"labs_kubernetes/helm/","title":"Helm","text":"<ul> <li>(Helm Oficial Page)[https://helm.sh/]</li> <li>Curso Youtube</li> <li>Artifact Hub</li> </ul>"},{"location":"labs_kubernetes/helm/#instalacion","title":"Instalacion","text":"<pre><code>$ apt install helm\n</code></pre>"},{"location":"labs_kubernetes/helm/#ejemplo-de-uso","title":"Ejemplo de uso","text":"<p>Instalando <code>nginx</code></p> <pre><code>$ helm install nginx\n</code></pre>"},{"location":"labs_kubernetes/helm/#helm-repos","title":"Helm Repos","text":"<p>Funcionan de manera analoga a los repositorios de paquetes de las distros de gnu linux, de donde bajar los paquetes que quieres instalar</p>"},{"location":"labs_kubernetes/helm/#agregar-un-repo","title":"Agregar un repo","text":"<p><pre><code>$ helm repo add &lt;custom_name&gt; &lt;url&gt;\n$ helm repo update\n</code></pre> Ejemplo<pre><code>$ helm repo add bitnami https://charts.bitnami.com/bitnami                  \n$ helm repo update\n</code></pre></p>"},{"location":"labs_kubernetes/helm/#listar-repos-agregados","title":"Listar repos agregados","text":"<pre><code>helm repo list\n    NAME                    URL                                               \n    strimzi                 https://strimzi.io/charts/                        \n    bitnami                 https://charts.bitnami.com/bitnami \n</code></pre>"},{"location":"labs_kubernetes/helm/graphana_prometheus/","title":"Instalando graphana y prometheus en un  cluster","text":"<ol> <li> <p>Buscar el paquete en artifacts hub: kube-prometheus-stack</p> </li> <li> <p>Instalar el paquete:</p> </li> </ol> <pre><code>$ helm repo add prom-repo https://prometheus-community.github.io/helm-charts\n    #\"prom-repo\" has been added to your repositories\n$ helm repo  update\n    # Hang tight while we grab the latest from your chart repositories...\n    # ...Successfully got an update from the \"prom-repo\" chart repository\n    # ...Successfully got an update from the \"prometheus-community\" chart repository\n    # ...Successfully got an update from the \"bitnami\" chart repository\n    # ...Successfully got an update from the \"strimzi\" chart repository\n    # Update Complete. \u2388Happy Helming!\u2388\n$  helm install my-kube-prometheus-stack prom-repo/kube-prometheus-stack\n    # NAME: my-kube-prometheus-stack\n    # LAST DEPLOYED: Tue Sep  6 12:43:31 2022\n    # NAMESPACE: default\n    # STATUS: deployed\n    # REVISION: 1\n    # NOTES:\n    # kube-prometheus-stack has been installed. Check its status by running:\n    #   kubectl --namespace default get pods -l \"release=my-kube-prometheus-stack\"\n\n    # Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.\n</code></pre> <ol> <li> <p>Ver todo lo que se instalo <pre><code>$ kubectl get all\n    NAME                                                               READY   STATUS    RESTARTS      AGE\n    pod/alertmanager-my-kube-prometheus-stack-alertmanager-0           2/2     Running   0             75s\n    pod/my-kube-prometheus-stack-grafana-f6f4b9bbf-4t6xx               3/3     Running   0             78s\n    pod/my-kube-prometheus-stack-kube-state-metrics-7db8d845ff-fw6ww   1/1     Running   0             78s\n    pod/my-kube-prometheus-stack-operator-5fc76fc95d-r8zgt             1/1     Running   0             78s\n    pod/my-kube-prometheus-stack-prometheus-node-exporter-pdsgh        1/1     Running   0             78s\n    pod/my-release-nginx-86445978c4-cqmgj                              1/1     Running   2 (38h ago)   5d19h\n    pod/nginx                                                          1/1     Running   4 (38h ago)   92d\n    pod/nginx-app-68cc968bbb-ljl99                                     1/1     Running   3 (38h ago)   6d20h\n    pod/prometheus-my-kube-prometheus-stack-prometheus-0               2/2     Running   0             74s\n\n    NAME                                                        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\n    service/alertmanager-operated                               ClusterIP      None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   75s\n    service/kubernetes                                          ClusterIP      10.96.0.1        &lt;none&gt;        443/TCP                      92d\n    service/my-kube-prometheus-stack-alertmanager               ClusterIP      10.103.81.182    &lt;none&gt;        9093/TCP                     78s\n    service/my-kube-prometheus-stack-grafana                    ClusterIP      10.109.202.64    &lt;none&gt;        80/TCP                       78s\n    service/my-kube-prometheus-stack-kube-state-metrics         ClusterIP      10.103.232.64    &lt;none&gt;        8080/TCP                     78s\n    service/my-kube-prometheus-stack-operator                   ClusterIP      10.104.93.232    &lt;none&gt;        443/TCP                      78s\n    service/my-kube-prometheus-stack-prometheus                 ClusterIP      10.97.75.96      &lt;none&gt;        9090/TCP                     78s\n    service/my-kube-prometheus-stack-prometheus-node-exporter   ClusterIP      10.97.206.26     &lt;none&gt;        9100/TCP                     78s\n</code></pre></p> </li> <li> <p>Para poder acceder al dashboard de grapaha se debe exponer el servicio <code>service/my-kube-prometheus-stack-grafana</code> en este caso con un <code>NodePort</code>, para eso hay que editar el servicio</p> </li> </ol> <pre><code>$ kubectl edit  svc my-kube-prometheus-stack-grafana\n</code></pre> <p>Se deben hacer las siguientes modificaciones: - Cambiar <code>spec.type</code> por <code>NodePort</code> - Agregar <code>spec.ports.nodeport</code> con el valor 30001, este atributo no funciona con los servicios <code>ClusterIP</code></p> Antes<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: my-kube-prometheus-stack\n    meta.helm.sh/release-namespace: default\n  creationTimestamp: \"2022-09-06T17:43:39Z\"\n  labels:\n    app.kubernetes.io/instance: my-kube-prometheus-stack\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/version: 9.0.5\n    helm.sh/chart: grafana-6.32.10\n  name: my-kube-prometheus-stack-grafana\n  namespace: default\n  resourceVersion: \"1054945\"\n  uid: 6be59343-45db-4dc3-a5d0-7f1f890baf92\nspec:\n  clusterIP: 10.109.202.64\n  clusterIPs:\n  - 10.109.202.64\n  externalTrafficPolicy: Cluster\n  internalTrafficPolicy: Cluster\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - name: http-web\n    port: 80\n    protocol: TCP\n    targetPort: 3000\n  type: ClusterIP \n  selector:\n    app.kubernetes.io/instance: my-kube-prometheus-stack\n</code></pre> Despues<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: my-kube-prometheus-stack\n    meta.helm.sh/release-namespace: default\n  creationTimestamp: \"2022-09-06T17:43:39Z\"\n  labels:\n    app.kubernetes.io/instance: my-kube-prometheus-stack\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/version: 9.0.5\n    helm.sh/chart: grafana-6.32.10\n  name: my-kube-prometheus-stack-grafana\n  namespace: default\n  resourceVersion: \"1054945\"\n  uid: 6be59343-45db-4dc3-a5d0-7f1f890baf92\nspec:\n  clusterIP: 10.109.202.64\n  clusterIPs:\n  - 10.109.202.64\n  externalTrafficPolicy: Cluster\n  internalTrafficPolicy: Cluster\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - name: http-web\n    port: 80\n    protocol: TCP\n    targetPort: 3000 # (1)\n    nodePort: 30001 # (2)\n  type: NodePort \n  selector:\n    app.kubernetes.io/instance: my-kube-prometheus-stack\n</code></pre> <ol> <li>obtener la ip de <code>minikube</code></li> </ol> <pre><code>$ minikube ip\n    # 192.168.39.104\n</code></pre> <ol> <li>abrir el navegador en la url <code>192.168.39.104:30001</code></li> </ol> <p></p>"},{"location":"labs_kubernetes/minikube/","title":"1. Instalacion","text":""},{"location":"labs_kubernetes/minikube/#preparacion-de-laboratorio-local-en-linux","title":"Preparacion de laboratorio local en linux","text":"<p>Herramientas necesarias:</p> <ul> <li>Kubectl <pre><code># download\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n# check checksum \ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\n# install \nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n# check installation\nkubectl version --client\n</code></pre></li> </ul> <ul> <li>Minikube <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n</code></pre></li> <li>Quemu Kvm <pre><code>sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager\nsudo systemctl is-active libvirtd\nsudo usermod -aG libvirt $USER\nsudo usermod -aG kvm $USER\n</code></pre></li> </ul>"},{"location":"labs_kubernetes/minikube/#minilab-levantar-nginx-en-un-pod-en-minikube","title":"Minilab Levantar NGINX en un pod en MINIKUBE","text":"<p>En este laboratorio se levantara un pod con un servicio de nginx y se expondra el puerto a la maquina local.</p> <pre><code># Create a cluster\nminikube start --driver=kvm2\n# start the pod running nginx\nkubectl create deployment --image=nginx nginx-app\n# [OPTIONAL] add env to nginx-app\nkubectl set env deployment/nginx-app  DOMAIN=cluster\n# expose a port through with a service\nkubectl expose deployment nginx-app --port=80 --name=nginx-http\n# [TEST] access to service\ncurl 127.0.0.1:8010\n</code></pre>"},{"location":"labs_kubernetes/minikube/#minilab-levantar-imagen-local-de-docker-en-kubernetes","title":"Minilab Levantar  imagen local de Docker en Kubernetes","text":"<p>En este laboratorio se creara una imagen docker de prueba  y se desplegara en un <code>pod</code>.</p> <ul> <li>Crear <code>Dockerfile</code>  que imprime en pantalla el mensaje <code>Hello World!</code>.</li> </ul> <pre><code>FROM busybox\nCMD [ \"echo\", \"Hello World!\"]\n</code></pre> <ul> <li>Crear la imagen Docker.</li> </ul> <pre><code>$ docker build . -t santos/hello-world\n    # Sending build context to Docker daemon  3.072kB\n    # Step 1/2 : FROM busybox\n    #  ---&gt; 62aedd01bd85\n    # Step 2/2 : CMD [ \"echo\", \"Hello World!\"]\n    #  ---&gt; Using cache\n    #  ---&gt; 79b4e6c1bd0d\n    # Successfully built 79b4e6c1bd0d\n    # Successfully tagged santos/hello-world:latest\n</code></pre> <ul> <li>Levantar un contenedor de prueba.</li> </ul> <pre><code>$ docker run santos/hello-world\n    # Hello World!\n</code></pre> <ul> <li>Crear el archivo de configuracion <code>helloworld.yml</code> para crear un job de kubernetes que corra el container  Docker. <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello-world\nspec:\n  template:\n    metadata:\n      name: hello-world-pod\n    spec:\n      containers:\n      - name: hello-world\n        image: santos/hello-world\n        imagePullPolicy: Never\n      restartPolicy: Never\n</code></pre> <code>restartPolicy</code> es <code>Never</code> debido que el container solo va a ejecutar el mensaje <code>Hello World!</code> y terminar, si se deja en <code>Always</code> se va a ejecutar infinitas veces.</li> </ul> <p><code>imagePullPolicy</code> es <code>Never</code> debido a que se quiere que busque la imagen localmente solamente, si no la encuentra no intentara bajarla del container registry.</p> <ul> <li> <p>Crear el job de kubernetes con este archivo de configuracion y <code>kubectl</code>. <pre><code>$ kubectl create -f helloworld.yml\n    # job.batch/hello-world created\n</code></pre></p> </li> <li> <p>Revisar si se ejecuto correctamente. <pre><code>$ kubectl get pods\n    # NAME              READY STATUS            RESTARTS AGE\n    # hello-world-r4g9g 0/1   ErrImageNeverPull 0        6s\n</code></pre></p> </li> </ul> <p>El error  <code>ErrImageNeverPull</code>  significa  que el nodo minikube usa su propio repositorio de Docker que no est\u00e1 conectado al registro de Docker en la m\u00e1quina local, por lo que, sin extraer, no sabe de d\u00f3nde obtener la imagen.</p> <p>Para solucionar esto, utilizo el comando <code>minikube docker-env</code> que genera las variables de entorno necesarias para apuntar el demonio local de Docker al registro interno de Docker de minikube:</p> <pre><code>$ minikube docker-env\n    # export DOCKER_TLS_VERIFY=\u201d1\"\n    # export DOCKER_HOST=\u201dtcp://172.17.0.2:2376\"\n    # export DOCKER_CERT_PATH=\u201d/home/user/.minikube/certs\u201d\n    # export MINIKUBE_ACTIVE_DOCKERD=\u201dminikube\u201d# To point your shell to minikube\u2019s docker-daemon, run:\n    ## eval $(minikube -p minikube docker-env)\n</code></pre> <ul> <li> <p>Aplicar las variables de <code>minikube</code> <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre></p> </li> <li> <p>Construyo la imagen de docker nuevamente, esta vez en el registro interno de <code>minukube</code>. <pre><code>$ docker build . -t santos/hello-world\n</code></pre></p> </li> <li> <p>Recrear el job otra vez. <pre><code>$ kubectl delete -f helloworld.yml\n$ kubectl create -f helloworld.yml\n</code></pre></p> </li> <li> <p>Revisar si se ejecuto correctamente. <pre><code>$ kubectl get pods\n    # NAME              READY STATUS            RESTARTS AGE\n    # hello-world-r4g9g 0/1   Completed         0        6s\n</code></pre></p> </li> <li> <p>Revisar los logs del pod. <pre><code>$ kubectl logs hello-world-f5hzz\n    # Hello World!\n</code></pre></p> </li> </ul>"},{"location":"labs_kubernetes/minikube/labs1/","title":"2. Load Balancer y una API Flask","text":"<p>En este laboratorio se desplegara una aplicacion dockerizada hecha en <code>python</code> en <code>kubernetes</code>, agregando un <code>LoadBalancer</code>.</p> <ol> <li> <p>Crear el app de python <pre><code>#save as app.py \nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"hello word from flask!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', debug=True)\n</code></pre></p> </li> <li> <p>Crear el <code>Dockerfile</code> <pre><code>FROM python:3.7\nRUN mkdir /app\nWORKDIR /app/\nADD . /app/\nRUN pip install flask\nCMD [\"python\", \"/app/app.py\"]\n</code></pre></p> </li> <li> <p>Construir la imagen, pero antes apuntar al registry de <code>minukube</code>.</p> </li> </ol> <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre> <pre><code>$ docker build . -t santos/flask-hello-world\n</code></pre> <ol> <li>Crear archivo de configuracion <code>deployment.yml</code></li> </ol> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: flask-test-service\nspec:\n  selector:\n    app: flask-test-app # en nombre de la aplicacion que va a gestionar\n  ports:\n  - protocol: \"TCP\"\n    port: 6000\n    targetPort: 5000 # El puerto donde se expone  la aplicacion Flask\n  type: LoadBalancer\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-test-app\nspec:\n  selector:\n    matchLabels:\n      app: flask-test-app\n  replicas: 5  # \n  template:\n    metadata:\n      labels:\n        app: flask-test-app\n    spec:\n      containers:\n      - name: flask-test-app\n        image: santos/flask-hello-world\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 5000\n</code></pre> <ol> <li> <p>Aplicar el archivo de configuracion con <code>kubectl</code>. <pre><code>$ kubectl create -f deployment.yml\n    # service/flask-test-service created\n    # deployment.apps/flask-test-app created\n</code></pre></p> </li> <li> <p>Revisar si se ejecuto correctamente. <pre><code>$ kubectl get pods\n  # NAME              READY STATUS            RESTARTS AGE\n  # flask-test-app-c6b649857-2tgs5     1/1     Running            0                14s\n  # flask-test-app-c6b649857-g7f28     1/1     Running            0                14s\n  # flask-test-app-c6b649857-lh4nb     1/1     Running            0                14s\n  # flask-test-app-c6b649857-rsdjr     1/1     Running            0                14s\n  # flask-test-app-c6b649857-vmsj5     1/1     Running            0                14s\n\n$ kubectl get services\n  # NAME                  TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\n  # flask-test-service    LoadBalancer   10.102.69.135    &lt;pending&gt;     6000:31902/TCP   81s\n</code></pre></p> </li> <li> <p>Obtener la url de acceso al LoadBalancer. Como se esta usando <code>minkube</code> se utiliza el siguiente comando.</p> </li> </ol> <p><pre><code>$ minikube service example-service --url\n  # http://192.168.39.232:31902\n</code></pre> Si no se esta usando minikube se utiliza <code>kubectl describe</code></p> <pre><code>$ kubectl describe services flask-test-service\n  # Name:                     flask-test-service\n  # Namespace:                default\n  # Labels:                   &lt;none&gt;\n  # Annotations:              &lt;none&gt;\n  # Selector:                 app=flask-test-app\n  # Type:                     LoadBalancer\n  # IP Family Policy:         SingleStack\n  # IP Families:              IPv4\n  # IP:                       10.102.69.135\n  # IPs:                      10.102.69.135\n  # LoadBalancer Ingress:     192.168.39.232\n  # Port:                     &lt;unset&gt;  6000/TCP\n  # TargetPort:               5000/TCP\n  # NodePort:                 &lt;unset&gt;  31902/TCP\n  # Endpoints:                172.17.0.12:5000,172.17.0.3:5000,172.17.0.6:5000 + 2 more...\n  # Session Affinity:         None\n  # External Traffic Policy:  Cluster\n  # Events:                   &lt;none&gt;\n</code></pre> <p>En la salida del comando se puede ver informacion del <code>LoadBalancer</code>, el primero que necesitamos es <code>LoadBalancer Ingress</code>.</p> <ol> <li>Hacer un get al API: <pre><code>$ curl  http://192.168.39.232:31902\n  # hello word from flask!  \n</code></pre></li> </ol>"},{"location":"labs_kubernetes/minikube/labs2/","title":"3. Compartir archivos entre  dos containers en un Pod","text":"<p>En este laboratorio se comunicaran dos <code>containers</code> que viven en un mismo <code>pod</code> usando <code>volumenes</code>.</p> <p></p> <ol> <li> <p>Crear el el <code>pod</code> con un <code>volumen</code> , y dos <code>containers</code> <pre><code># save as ./two-containers-and-volume.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n\n  restartPolicy: Never\n\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n\n  containers:\n\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"echo Hello from the debian container &gt; /pod-data/index.html\"]\n</code></pre> Como se puede ver se crea un <code>Pod</code> llamado <code>two-containers</code> y dentro de la especificacion se crea un <code>volumen</code> llamado <code>shared-data</code> , luego se crea un <code>container</code> llamado <code>nginx-container</code> montando el volumen en  <code>/usr/share/nginx/html</code>, y por ultimo se crea el <code>container</code> llamado <code>debian-container</code>, montando el volumen en  <code>/pod-data</code>. El segundo container ejecutara el siguiente comando y  terminara su ejecucion. <pre><code>$ echo Hello from the debian container &gt; /pod-data/index.html\n</code></pre> El segundo container modificara el archivo <code>index.html</code> de la carpeta principal del servidor <code>nginx</code> del segundo container, ya que este esta en el volumen compartido.</p> </li> <li> <p>Crear el <code>pod</code> usando <code>kubectl</code> <pre><code>$ kubectl apply -f ./two-containers-and-volume.yaml \n  # pod/two-containers created\n</code></pre></p> </li> <li> <p>Para ver la informacion detallada del <code>pod</code></p> </li> </ol> <p><pre><code>$ kubectl get pod two-containers --output=yaml\n</code></pre> output kubectl get pod two-containers<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/usr/share/nginx/html\",\"name\":\"shared-data\"}]},{\"args\":[\"-c\",\"echo Hello from the debian container \\u003e /pod-data/index.html\"],\"command\":[\"/bin/sh\"],\"image\":\"debian\",\"name\":\"debian-container\",\"volumeMounts\":[{\"mountPath\":\"/pod-data\",\"name\":\"shared-data\"}]}],\"restartPolicy\":\"Never\",\"volumes\":[{\"emptyDir\":{},\"name\":\"shared-data\"}]}}\n  creationTimestamp: \"2022-06-16T14:26:55Z\"\n  name: two-containers\n  namespace: default\n  resourceVersion: \"35653\"\n  uid: a2f25f74-e860-465d-b0e9-1fbdfa9d8a1a\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: Always\n    name: nginx-container\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-mvrgj\n      readOnly: true\n  - args:\n    - -c\n    - echo Hello from the debian container &gt; /pod-data/index.html\n    command:\n    - /bin/sh\n    image: debian\n    imagePullPolicy: Always\n    name: debian-container\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /pod-data\n      name: shared-data\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-mvrgj\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: minikube\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Never\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - emptyDir: {}\n    name: shared-data\n  - name: kube-api-access-mvrgj\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    status: \"True\"\n    type: Initialized\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    message: 'containers with unready status: [debian-container]'\n    reason: ContainersNotReady\n    status: \"False\"\n    type: Ready\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    message: 'containers with unready status: [debian-container]'\n    reason: ContainersNotReady\n    status: \"False\"\n    type: ContainersReady\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-16T14:26:55Z\"\n    status: \"True\"\n    type: PodScheduled\n  containerStatuses:\n  - containerID: docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5\n    image: debian:latest\n    imageID: docker-pullable://debian@sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510\n    lastState: {}\n    name: debian-container\n    ready: false # (1)\n    restartCount: 0\n    started: false\n    state:\n      terminated:\n        containerID: docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5\n        exitCode: 0\n        finishedAt: \"2022-06-16T14:27:00Z\"\n        reason: Completed # (2)\n        startedAt: \"2022-06-16T14:27:00Z\"\n  - containerID: docker://00c92009ff5b762f5a26552f1515addbe118403ab4efbb1e2d1244c698f0ccb2\n    image: nginx:latest\n    imageID: docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514\n    lastState: {}\n    name: nginx-container\n    ready: true\n    restartCount: 0\n    started: true\n    state:\n      running:\n        startedAt: \"2022-06-16T14:26:57Z\"\n  hostIP: 192.168.39.232\n  phase: Running # (3)\n  podIP: 172.17.0.16\n  podIPs:\n  - ip: 172.17.0.16\n  qosClass: BestEffort\n  startTime: \"2022-06-16T14:26:55Z\"\n</code></pre></p> <ol> <li> <p>Es <code>false</code>  debido a que el contenedor solo ejecuta un <code>echo</code> y termina su ejecucion.</p> </li> <li> <p>El <code>container</code> termino su ejecucion con <code>exito</code>.</p> </li> <li>El <code>container</code> nginx esta corriendo porque es un <code>servidor</code>.</li> </ol> <p>Se puede ver que container <code>debian-container</code> termino su ejecucion y <code>nginx-container</code> sigue corriendo.</p> <ol> <li>Entrar al contenedor <code>nginx-container</code> <pre><code>$ kubectl exec -it two-containers -c nginx-container -- /bin/bash\n</code></pre></li> <li> <p>Revisar el archivo <code>index.html</code> de la carpeta compartida <pre><code>root@two-containers:/# cat /usr/share/nginx/html/index.html \n  # Hello from the debian container\n</code></pre></p> </li> <li> <p>Revisar la respuesta del servidor de <code>nginx</code> <pre><code>root@two-containers:/# curl localhost\n  # Hello from the debian container\n</code></pre></p> </li> </ol> <pre><code>{\"data\":{\"attributes\":{\"resource_names_by_hash\":{\"7d74f4592a9fd6ff\":\"POST businessloki.loki.svc.cluster.local:3100/loki/api/v1/push\",\"5fc09ce97131f406\":\"POST dc.services.visualstudio.com/v2/track\",\"424ed4ae196ebbaf\":\"POST rt.services.visualstudio.com/QuickPulseService.svc/ping\"}},\"type\":\"resource_names_by_hash\"}}\n</code></pre>"},{"location":"labs_kubernetes/minikube/labs3/","title":"4. Ping desde entre dos containers de un mismo POD","text":"<p>En este laboratorio se mostrara que se pueden alcanzar containers en un mismo pod. En este laboratorio se usara la imagen <code>santos/flask-hello-world</code> de laboratorios atras, y tambien usara una configuracion sencilla nginx.</p> <ol> <li> <p>Crear el archivo de configuracion de nginx <code>nginx-vol/default.conf</code> el el directorio que vamos a enlazar con un volumen al container. <pre><code>server {\n    listen       80;\n    listen  [::]:80;\n    server_name  localhost;\n\n    location /flask_app {\n        proxy_pass http://127.0.0.1:5000/;   \n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n}\n</code></pre> Esta pequena configuracion sencilla de <code>ngnx</code> crea un <code>proxy_pass</code> de <code>localhost/flask_app</code> a <code>localhost:5000</code>, este es el puerto de escucha de la aplicacion de <code>flask</code> que se usara en este laboratorio.</p> </li> <li> <p>Recordar que kubernetes no tiene comunicacion directa con nuestra maquina, tiene comunicacion directa con la maquina virtual que monta minikube, como se quiere montar un directorio de nuestra maquina como un volumen, primero se debe montar en la maquina virtual de minikube. <pre><code>minikube mount nginx-vol/:/mynginx/\n# \ud83d\udcc1  Mounting host path myvol/ into VM as /mynginx/ ...\n#     \u25aa Mount type:   \n#     \u25aa User ID:      docker\n#     \u25aa Group ID:     docker\n#     \u25aa Version:      9p2000.L\n#     \u25aa Message Size: 262144\n#     \u25aa Options:      map[]\n#     \u25aa Bind Address: 192.168.39.1:40637\n# \ud83d\ude80  Userspace file server: ufs starting\n# \u2705  Successfully mounted myvol/ to /mynginx/\n\n# \ud83d\udccc  NOTE: This process must stay alive for the mount to be accessible ...\n</code></pre> El volumen estara montado mientras no terminemos este proceso. Ahora existe una carpeta llamada <code>/mynginx</code> en la <code>VM</code>.</p> </li> <li> <p>Yaml de creacion del <code>Pod</code> <pre><code># save as ./spec.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers-ping\nspec:\n\n  restartPolicy: Never\n  volumes:\n  - name: shared-data\n    hostPath:\n      path: /mynginx/\n      type: Directory\n\n  containers:\n\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /etc/nginx/conf.d\n  - name: flask-test-app\n    image: santos/flask-hello-world\n    imagePullPolicy: Never\n    # ports:\n    # - containerPort: 5000\n</code></pre></p> </li> <li> <p>Crear el <code>pod</code> usando <code>kubectl</code> <pre><code>$ kubectl apply -f ./spec.yaml \n  # pod/two-containers-ping created\n</code></pre></p> </li> <li> <p>Revisar el estado del nuevo <code>pod</code> <pre><code>kubectl get pod two-containers-ping --output=yaml\n</code></pre></p> </li> </ol> <p>output kubectl get pod<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers-ping\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/conf.d\",\"name\":\"shared-data\"}]},{\"image\":\"santos/flask-hello-world\",\"imagePullPolicy\":\"Never\",\"name\":\"flask-test-app\"}],\"restartPolicy\":\"Never\",\"volumes\":[{\"hostPath\":{\"path\":\"/mynginx/\",\"type\":\"Directory\"},\"name\":\"shared-data\"}]}}\n  creationTimestamp: \"2022-06-17T14:38:29Z\"\n  name: two-containers-ping\n  namespace: default\n  resourceVersion: \"52327\"\n  uid: 6dbb4caa-87b3-498a-8c4a-a7aeb76842af\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: Always\n    name: nginx-container\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /etc/nginx/conf.d\n      name: shared-data\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-8vpjq\n      readOnly: true\n  - image: santos/flask-hello-world\n    imagePullPolicy: Never\n    name: flask-test-app\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-8vpjq\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: minikube\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Never\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - hostPath:\n      path: /mynginx/\n      type: Directory\n    name: shared-data\n  - name: kube-api-access-8vpjq\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:29Z\"\n    status: \"True\"\n    type: Initialized\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:31Z\"\n    status: \"True\"\n    type: Ready\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:31Z\"\n    status: \"True\"\n    type: ContainersReady\n  - lastProbeTime: null\n    lastTransitionTime: \"2022-06-17T14:38:29Z\"\n    status: \"True\"\n    type: PodScheduled\n  containerStatuses:\n  - containerID: docker://0b53b339b7c06b7475739cbde17795ce24f71bc8d7d2e41da07d220162b6475d\n    image: santos/flask-hello-world:latest\n    imageID: docker://sha256:105bc09b0a7e87f7344040742f1fa011e00e7a36bac1f1cabc8a5303ceb1db30\n    lastState: {}\n    name: flask-test-app\n    ready: true\n    restartCount: 0\n    started: true\n    state:\n      running:\n        startedAt: \"2022-06-17T14:38:31Z\"\n  - containerID: docker://8175ac7cd60060159888a723b7a36a93486a38e9b6cd4379d7e86fbb4647375d\n    image: nginx:latest\n    imageID: docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514\n    lastState: {}\n    name: nginx-container\n    ready: true\n    restartCount: 0\n    started: true\n    state:\n      running:\n        startedAt: \"2022-06-17T14:38:31Z\"\n  hostIP: 192.168.39.232\n  phase: Running\n  podIP: 172.17.0.16\n  podIPs:\n  - ip: 172.17.0.16\n  qosClass: BestEffort\n  startTime: \"2022-06-17T14:38:29Z\"\n</code></pre> Como se puede ver los dos containers estan <code>Ready</code></p> <ol> <li> <p>Entrar al container   <code>nginx-container</code> <pre><code>$ kubectl exec -it two-containers-ping -c nginx-container -- /bin/bash\n  # root@two-containers-ping:/\n</code></pre></p> </li> <li> <p>Hacer un get a <code>http://localhost/flask_app/</code> <pre><code>$root@two-containers-ping:  curl http://localhost/flask_app\n  # hello word from flask!\n</code></pre></p> </li> </ol>"},{"location":"labs_kubernetes/minikube/labs3/#conclusion","title":"Conclusion","text":"<p>Containers within a pod share an IP address and port space, and can find each other via localhost s</p>"},{"location":"labs_kubernetes/minikube/labs4/","title":"5. Comunicacion de dos PODS con un service NodePort","text":"<p>En este laboratorio se creara un servicio NodePort para que un pod pueda alcanzar a otro usando  un <code>SVCHOST</code>. </p> <ol> <li> <p>Para este laboratorio se creo una apliacion en flask que se conecta a una base de datos mongo, esta app tiene dos rutas, enviar mensaje, leer mensajes, los mensajes se guardan en la base de datos. <pre><code>#save as app.py \nfrom flask import Flask\nfrom flask import jsonify\nfrom pymongo import MongoClient\napp = Flask(__name__) \n# se usa el host del Servicio NodePort\nclient = MongoClient(\"mongo.default.svc.cluster.local\",27017)\ndb = client.test_db\n@app.route(\"/\")\ndef hello():\n    return \"works\"\n\n@app.route(\"/send_message/&lt;sender&gt;/&lt;to&gt;/&lt;message&gt;\")\ndef send_message(sender,to,message):\n    return str(db.messages.insert_one({'to':to,'message':message,'from':sender}).inserted_id)\n@app.route(\"/get_messages/&lt;to&gt;\")\ndef get_messages(to):\n    messages =  list(db.messages.find({\"to\":to},{\"_id\":0}))\n\n    if messages:\n        db.messages.delete_many({\"to\":to})   \n    print(messages)\n    return jsonify(messages     )\n</code></pre></p> </li> <li> <p>Escribir el <code>Dockerfile</code> <pre><code>FROM python:3.7\nRUN mkdir /app\nWORKDIR /app/\nRUN pip install flask==2.0.0 pymongo\n\nADD . /app/\n\nENV FLASK_APP=app.py\n\nENTRYPOINT [ \"flask\"]\nCMD [ \"run\", \"--host\", \"0.0.0.0\",\"-p\", \"5000\" ]\n</code></pre></p> </li> <li> <p>Crear la imagen <code>santos/flask-messages-app-with-mongo</code> a partir del <code>Dockerfile</code>, apuntar al registry de <code>minikube</code>.</p> </li> </ol> <p><pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre> <pre><code>$ docker build . -t santos/flask-messages-app-with-mongo\n</code></pre></p> <ol> <li> <p>Yaml de creacion de los dos <code>pods</code> usando <code>Deployments</code> <pre><code># save as ./deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-messages\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: flask-messages\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: flask-messages\n    spec:\n      containers:\n      - name: flask-messages\n        image:  santos/flask-messages-app-with-mongo\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 5000\n\n---\n# Deploy de la db\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image:  mongo\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 27017\n</code></pre></p> </li> <li> <p>Ejecutar el <code>deploment.yaml</code> <pre><code>$ kubectl apply -f ./deployment.yaml \n# deployment.apps/flask-messages created\n# deployment.apps/mongo created\n</code></pre></p> </li> <li> <p>Revisar el estado del cluster <pre><code>$ kubectl get pods\n\n  # flask-messages-b54fdfbb-2kkrl   1/1     Running   0          40m\n  # mongo-7f4df74f64-mdkgm          1/1     Running   0          40m\n\n$ kubectl get deployments\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\nflask-messages   1/1     1            1           41m\nmongo            1/1     1            1           41m\n</code></pre></p> </li> <li> <p>Yaml de creacion de servicios, uno para que la aplicacion pueda acceder a la db con un <code>hostname</code> y otra para poder acceder a la aplicacion desde la maquina local.</p> </li> </ol> <pre><code>#save as ./services.yaml\n\n\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: mongo\n  namespace: default\nspec:\n  ports:\n  - port: 27017\n    protocol: TCP\n    targetPort: 27017\n  selector:\n    app: mongo\n  type: NodePort\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: flask-messages\n  namespace: default\nspec:\n  ports:\n  - port: 5000\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: flask-messages\n  type: NodePort\n</code></pre> <ol> <li> <p>Crear los servicios <code>services.yaml</code> <pre><code>$ kubectl apply -f ./services.yaml \n  # service/mongo created\n  # service/flask-messages created\n</code></pre></p> </li> <li> <p>Revisar el estado del cluster</p> </li> </ol> <p><pre><code>$ kubectl get services\n  # NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n  # flask-messages   NodePort    10.109.12.181   &lt;none&gt;        5000:31377/TCP    46m\n  # kubernetes       ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP           96m\n  # mongo            NodePort    10.97.20.81     &lt;none&gt;        27017:31644/TCP   46m\n</code></pre> 7. Recordar  que kubernetes esta corriendo en una maquina virtual, por ende para poder acceder al servicio  <code>flask-messages</code> se debe obtener la url de acceso usando minikube <pre><code>$ minikube service flask-messages  --url \n  #  http://192.168.39.95:31377\n</code></pre></p> <ol> <li>Usar la aplicacion </li> </ol> <p><pre><code># Enviar mensajes\ncurl http://192.168.39.95:31377/new_message/santos/juan/holajuan\n# 62ae84f9ab010d71976d6c2a\ncurl http://192.168.39.95:31377/new_message/santos/juan/estoyenviandounmensajedesdekubernetes\n# 62ae8531ab010d71976d6c2b\n</code></pre> Leer mensajes <pre><code>$ curl http://192.168.39.95:31377/get_messages/juan\n</code></pre> <pre><code>[\n    {\n        \"from\": \"santos\",\n        \"message\": \"holajuan\",\n        \"to\": \"juan\"\n    },\n    {\n        \"from\": \"santos\",\n        \"message\": \"estoyenviandounmensajedesdekubernetes\",\n        \"to\": \"juan\"\n    }\n]\n</code></pre></p>"},{"location":"labs_kubernetes/minikube/labs5/","title":"6. Docker Coins en Kubernetes","text":"<p>En este laboratorio montara la conocida  aplicacion <code>Docker Coins</code> en Kubernetes, especificamente se clono este repositorio <code>Docker Coins</code></p> <p></p> <p>Cada microservicio(<code>worker</code>,<code>rng</code>,<code>webui</code>,<code>hasher</code>) y redis se montara en <code>Pods</code> diferentes. 1. Clonar el repo: <pre><code>git clone https://github.com/platzi/curso-kubernetes\n# dockercoins projects is in dockercoins folder\n</code></pre> 2. Como se ve en el diagrama de arquitectura,  <code>worker</code> y  <code>webui</code> son los unicos microservicios que acceden a otros, se deben modificar las urls de acceso a los otros microservicios, para colocar las urls <code>svc</code> de kubernetes, ya que se usaran servicios <code>NodePort</code> para poder acceder a los microservicios, con los nombres: <code>redis</code>,<code>rng</code>,<code>hasher</code> respectivamente.</p> <p><pre><code>### dockercoins/webui/webui.js line 5\n- var client = redis.createClient(6379, 'redis');\n+ var client = redis.createClient(6379, 'redis.default.svc.cluster.local');\n</code></pre> <pre><code>### dockercoins/worker/worker.py line 17\n- redis = Redis(\"redis\")\n+ redis = Redis(\"redis.default.svc.cluster.local\")\n</code></pre> <pre><code>### dockercoins/worker/worker.py line 21\n- r = requests.get(\"http://rng/32\")\n+ r = requests.get(\"http://rng.default.svc.cluster.local/32\")\n</code></pre> <pre><code>### dockercoins/worker/worker.py line 26,27,28\n- r = requests.post(\"http://hasher/\",\n-                      data=data,\n-                      headers={\"Content-Type\": \"application/octet-stream\"})\n+ r = requests.post(\"http://hasher.default.svc.cluster.local/\",\n+                      data=data,\n+                      headers={\"Content-Type\": \"application/octet-stream\"})\n</code></pre></p> <p>Notar que:</p> <ul> <li>En nodejs hay que poner la url sin <code>http://</code>.</li> <li>En python si hay que colocarlo en las urls que se consumen con la libreria <code>requests</code>.</li> <li>La url de  <code>redis</code> no lleva el protocolo:  <code>redis.default.svc.cluster.local</code>. </li> </ul> <ol> <li>Crear las imagenes de docker, por facilidad se usara docker-compose para generar todas las imagenes con un comando. Antes apuntar al registry de <code>minikube</code> <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre> <pre><code>## TODO ignore redis build\n$ docker-compose  -f dockercoins/docker-compose.yml  build\n</code></pre></li> <li>Renombrar las imagenes</li> </ol> <p><pre><code>docker tag dockercoins_worker  santos/docker-coins-worker-app\ndocker tag dockercoins_hasher santos/docker-coins-hasher-app\ndocker tag dockercoins_rng:latest   santos/docker-coins-rng-app\ndocker tag dockercoins_webui santos/docker-coins-web-ui-app\n</code></pre> 5. Crear los archivos de depliegue de <code>kubernetes</code> config.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-configmap\n  namespace: default\ndata:\n  redis.conf: |\n    protected-mode no\n    maxmemory 32mb\n    maxmemory-policy allkeys-lru\n# cat /usr/local/etc/redis/redis.conf \n\n---\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-sysctl\n  namespace: default\ndata:\n  sysctl.conf: |\n    net.core.somaxconn=511\n    vm.overcommit_memory=1\n    net.core.somaxconn=4096\n</code></pre> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: web-ui\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: web-ui\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: web-ui\n        image:  santos/docker-coins-web-ui-app\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: worker\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: worker\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: worker\n        image:  santos/docker-coins-worker-app\n        imagePullPolicy: Never\n        # ports:\n        # - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: redis\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: redis\n        image:  redis\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        volumeMounts:\n        - mountPath: /usr/local/etc/redis/redis.conf\n          name: redis-configmap\n          subPath: redis.conf\n\n        - mountPath: /etc/sysctl.conf\n          name: redis-sysctl\n          subPath: sysctl.conf\n        command: [\"redis-server\", \"/usr/local/etc/redis/redis.conf\"]\n      volumes:\n      - name: redis-configmap\n        configMap:\n          name: redis-configmap\n      - name: redis-sysctl\n        configMap:\n          name: redis-sysctl\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hasher\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: hasher\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: hasher\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: hasher\n        image:  santos/docker-coins-hasher-app\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rng\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: rng\n      masterapp: dockercoins\n  replicas: 1  # \n  template:\n    metadata:\n      labels:\n        app: rng\n        masterapp: dockercoins\n    spec:\n      containers:\n      - name: rng\n        image:  santos/docker-coins-rng-app\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n</code></pre> services.yaml<pre><code>#save as ./services.yaml\n\n\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: web-ui\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: web-ui\n  type: NodePort\n---\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: redis\n  namespace: default\nspec:\n  ports:\n  - port: 6379\n    # protocol: TCP\n    targetPort: 6379\n  selector:\n    app: redis\n  type: NodePort\n---\napiVersion: v1\nkind: Service\nmetadata:\n\n  name: hasher\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: hasher\n  type: NodePort\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name:  rng\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app:  rng\n  type: NodePort\n</code></pre> Notar que en los deployments hay una etiqueta llamada <code>masterapp</code> con el valor dockercoins, la cual servira para hacer operaciones sobre todos los deployments.  6. Applicar los archivos de configuracion <pre><code>$ kubectl apply -f config.yaml &amp;&amp; kubectl apply -f services.yaml \\\n &amp;&amp; kubectl apply -f deployment.yaml\n  # configmap/redis-configmap created\n  # configmap/redis-sysctl created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n  # deployment.apps/web-ui created\n  # deployment.apps/worker created\n  # deployment.apps/redis created\n  # deployment.apps/hasher created\n  # deployment.apps/rng created\n  # pod/dnsutils created\n  # service/web-ui created\n  # service/redis created\n  # service/hasher created\n  # service/rng created\n</code></pre> 7. Ver los pods <pre><code>NAME                            READY   STATUS    RESTARTS      AGE\nhasher-6fd7cfd7bf-8v86t         1/1     Running   0             7m51s\nredis-594c4d767f-8xfrd          1/1     Running   0             7m51s\nrng-648c7c46c7-qrn2m            1/1     Running   0             7m51s\nweb-ui-57b7674785-7cfj2         1/1     Running   0             7m51s\nworker-67ff756f89-wvmt4         1/1     Running   0             7m51s\n</code></pre> 8. Ver los logs de todos los deployments</p> <p><pre><code>$ kubectl logs --selector masterapp=dockercoins\n</code></pre> <pre><code>1:C 21 Jun 2022 20:47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 21 Jun 2022 20:47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 21 Jun 2022 20:47:44.842 # Configuration loaded\n1:M 21 Jun 2022 20:47:44.842 * monotonic clock: POSIX clock_gettime\n1:M 21 Jun 2022 20:47:44.843 * Running mode=standalone, port=6379.\n1:M 21 Jun 2022 20:47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Jun 2022 20:47:44.843 # Server initialized\n1:M 21 Jun 2022 20:47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0c7d1044...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:Coin found: 0fe2be6c...\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\nINFO:__main__:4 units of work done, updating hash counter\n\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1008\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1006\n172.17.0.1 - - [21/Jun/2022:20:51:50 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n172.17.0.1 - - [21/Jun/2022:20:51:51 +0000] \"POST / HTTP/1.1\" 200 64 0.1007\n\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:03] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n172.17.0.1 - - [21/Jun/2022 20:52:04] \"GET /32 HTTP/1.1\" 200 -\n\n\nWEBUI running on port 80\n</code></pre> 9. Exponer el servicio <code>NodePort</code> de <code>web-ui</code> con minikube  <pre><code>$ minikube service web-ui --url\n  # http://192.168.39.95:31108\n</code></pre> 10. Abrir el link en el navegador</p> <p></p>"},{"location":"labs_kubernetes/minikube/labs5/#posibles-errores","title":"Posibles errores","text":"<ol> <li>Resolucion de <code>hostnames</code> <code>svc</code> en el microservicio <code>worker</code> o <code>webui</code>, puede ser porque se creo el servicio <code>NodePort</code> de los otros microservicios despues de que se ejecutaran los deplyments,  es decir no existian esos hostnames, para esto se puede reiniciar el deployment fallido.</li> </ol> <p><pre><code>$ kubectl rollout restart deployment worker\n  # deployment.apps/worker restarted\n\n$ kubectl rollout restart deployment web-ui\n  # deployment.apps/web-ui restarted\n</code></pre> 2. <code>Connection refused</code> o <code>ENOTFOUND</code>, recordar que:</p> <ul> <li>En nodejs hay que poner la url sin <code>http://</code>.</li> <li>En python si hay que colocar el protocolo en  las urls que se consumen con la libreria <code>requests</code>.</li> <li>La url de  <code>redis</code> no lleva el protocolo:  <code>redis.default.svc.cluster.local</code>.</li> <li><code>ImageNeverPull</code>, no se apunto al registry de <code>minikube</code>: <pre><code>$ eval $(minikube -p minikube docker-env)\n</code></pre></li> </ul>"},{"location":"labs_kubernetes/minikube/labs5/#mira-este-lab-en-azure-kubernetes-service","title":"Mira este lab en Azure Kubernetes Service:","text":"<p>En la siguiente pagina podras ver el laboratio link</p>"},{"location":"linkedin/python/","title":"Python Aptitude Test","text":"<p>Questions and answers</p> <ol> <li> <p>Which statement about class methods is true?</p> <ul> <li>A class methos is a regular funciton that belongs to  a class, but it must return None</li> <li>A class method is a similar to an regular funcion, but a class method does not take any arguments.</li> <li>[x]  A class method can modify the state of the class, but it cannot directly modify the state of an instance of that class</li> <li>A class method holds all of the data for a particular class.</li> </ul> </li> <li> <p>Which statement about static class methods is true?</p> <ul> <li>Static  methods can be bound  to either  a class or an instance of a class.</li> <li>[x] Static class methods serve mostly as utility  or helper methods, since they  cannot  access  or modify  a class\u2019s state.</li> <li>Static class methods can access  and modify  the state of a class  or an instance  of a class.</li> <li>Static class methods are called static because  they  always return None.</li> </ul> </li> <li> <p>How does <code>defaultdict</code>  work?</p> <ul> <li>[x] if you try to read  from <code>defaultdict</code> with a  noneexistent key,  a new default key-value pair will be  created  for you  instead throwing  a <code>KeyError</code>.</li> <li><code>defaultdict</code> stores  a copy of a dictionary  in memory  that you  can default  to if  the original  gets unintentionally  modified.</li> <li><code>defaultdict</code> will automatically create a dictionary  for you  that has  keys  wich are  the integers  0-10.</li> <li><code>defaultdict</code> forces a dictionary  to only  accept keys  that are  of the data  type  specified  when  you created  the <code>defaultdict</code> (such as strings  or    integers).</li> </ul> </li> <li> <p>What is  the correct  syntax  for creating  a variable  that bound a set?</p> <ul> <li>[x] <code>my_set = {0, 'apple',3.5}</code></li> <li><code>my_set = to_set(0, 'apple',3.5)</code></li> <li><code>my_set = (0, 'apple',3.5).set()</code></li> <li><code>my_set = (0, 'apple',3.5).to_set()</code></li> </ul> </li> <li> <p>What is an abstract class?</p> <ul> <li>Abstract  classes  must  be redefined any time  an object  is instantiated  from  them.</li> <li>[x] An  Abstract  class exist only  so that  other  \u201cconcrete\u201d classes can  inherit  from  the  abstract class.</li> <li>Abstract  classes  must inherit  from concrete classes.</li> <li>An  Abstract  class is the name  for any class  from which  you can instantiate  an object.</li> </ul> </li> <li> <p>What is the purpose of the pass statement in Python?</p> <ul> <li>It is used to skip the yield statement of a generator and return a value of None.</li> <li>[x] It is a null operation used mainly as a placeholder in functions, classes, etc.</li> <li>It is used to pass control from one statement block to another.</li> <li>It is used to skip the rest of a while or for loop and return to the start of the loop.</li> </ul> </li> <li> <p>What is the diference between class attributes and instance atributes</p> <ul> <li>Class attributes belong just  to the class , not  to instances  of that class. Instance  attributes  are shared among  all instances  of  a class.</li> <li>Class attributes </li> </ul> </li> </ol>"}]}